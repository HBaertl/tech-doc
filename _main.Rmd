---
title: "Technical Documentation, State of the Ecosystem Report"
author: "Northeast Fisheries Science Center"
date: "`r format(Sys.Date(), '%e %B %Y')`"
site: bookdown::bookdown_site
output: 
  bookdown::gitbook
  bookdown::pdf_book
documentclass: book
bibliography: ["bibliography/introduction.bib", "bibliography/conceptmods.bib", "bibliography/trend_analysis.bib", "bibliography/EPU.bib", "bibliography/survey_data.bib", "bibliography/Bennet_indicator.bib", "bibliography/Revenue_Diversity.bib", "bibliography/Aquaculture.bib", "bibliography/NE_HAB.bib", "bibliography/MAB_HAB.bib", "bibliography/Comm_rel_vuln.bib", "bibliography/Comm_climate_vuln.bib", "bibliography/RW.bib", "bibliography/thermal_hab_proj.bib", "bibliography/occupancy.bib", "bibliography/long_term_sst.bib", "bibliography/seasonal_sst_anomaly_maps.bib", "bibliography/Ich_div.bib", "bibliography/Species_dist.bib", "bibliography/CHL_PPD.bib", "bibliography/zooplankton.bib", "bibliography/Condition.bib", "bibliography/productivity_tech_memo.bib","bibliography/aggregate_groups.bib"]
biblio-style: apalike
link-citations: yes
github-repo: NOAA-EDAB/tech-doc
description: "This book documents each indicator and analysis used in State of the Ecosystem reporting"
---
# Introduction {-}

The purpose of this document is to collate the methods used to access, collect, process, and analyze derived data ("indicators") used to describe the status and trend of social, economical, ecological, and biological conditions in the Northeast Shelf Large Marine Ecosystem (see figure, below). These indicators are further synthesized in State of the Ecosystem Reports produced annually by the [Northeast Fisheries Science Center](https://www.nefsc.noaa.gov/) for the [New England Fisheries Management Council](https://www.nefmc.org/) and the [Mid-Atlantic Fisheries Management Council](http://www.mafmc.org/). The metadata for each indicator (in accordance with the [Public Access to Research Results (PARR) directive](http://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/ostp_public_access_memo_2013.pdf)) and the methods used to construct each indicator are described in the subsequent chapters, with each chapter title corresponding to an indicator or analysis present in State of the Ecosystem Reports.

Indicators included in this document were selected to clearly align with management objectives, which is required for integrated ecosystem assessment [@levin_integrated_2009], and has been advised many times in the literature [@degnbol_review_2004; @jennings_indicators_2005; @rice_framework_2005; @link_translating_2005]. A difficulty with pratical implementation of this in ecosystem reporting can be the lack of clearly specified ecosystem-level management objectives (although some have been suggested [@murawski_definitions_2000]). In our case, considerable effort had already been applied to derive both general goals and operational objectives from both US legislation such as the Magnuson-Stevens Fisheries Conservation and Management Act (MSA) and regional sources [@depiper_operationalizing_2017]. These objectives are somewhat general and would need refinement together with managers and stakeholders, however, they serve as a useful starting point to structure ecosystem reporting.

```{r setup, echo=F, eval=T}
#source directories
image.dir <- here::here("images")
r.dir <- here::here("R")
gis.dir <- here::here("gis")

#Plotting and data libraries
library(ggplot2)
library(dplyr)
library(tidyr)
library(ecodata)
library(here)
library(kableExtra)
library(ggrepel)
library(stringr)
library(patchwork)
library(grid)
library(vegan)
library(rpart)
library(knitr)
library(rmarkdown)
library(magick)
library(readr)

#GIS libraries
library(sf)
library(rgdal)
library(raster)
library(rnaturalearth)

#Time series constants
shade.alpha <- 0.3
shade.fill <- "lightgrey"
lwd <- 1
pcex <- 2
trend.alpha <- 0.5
trend.size <- 2
hline.size <- 1
hline.alpha <- 0.35
hline.lty <- "dashed"
label.size <- 5
hjust.label <- 1.5
letter_size <- 4
feeding.guilds <- c("Apex Predator","Piscivore","Planktivore","Benthivore","Benthos")
x.shade.min <- 2009
x.shade.max <- 2018

#Function for custom ggplot facet labels
label <- function(variable,value){
  return(facet_names[value])
}

#Map line parameters
map.lwd <- 0.4

#CRS
crs <- "+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0 +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"

#Coastline shapefile
coast <- ne_countries(scale = 10,
                          continent = "North America",
                          returnclass = "sf") %>%
             sf::st_transform(crs = crs)

#State polygons
ne_states <- ne_states(country = "united states of america",
                                      returnclass = "sf") %>%
  sf::st_transform(crs = crs)

#high-res polygon of Maine
#new_england <- read_sf(gis.dir,"new_england")

#EPU shapefile
epu_sf <- ecodata::epu_sf %>% 
  filter(EPU %in% c("MAB","GB","GOM"))


```
Map of Northeast U.S. Continental Shelf Large Marine Ecosystem from @Hare2016.


```{r neusmap, fig.align='center', fig.height=6, echo = F}
knitr::include_graphics(file.path(image.dir, "journal.pone.0146756.g002.png"))

```

<!-- **References** -->

<!-- Hare JA, Morrison WE, Nelson MW, Stachura MM, Teeters EJ, Griffis RB, et al. (2016) A Vulnerability Assessment of Fish and Invertebrates to Climate Change on the Northeast US Continental Shelf. PLoS ONE 11(2): e0146756. https://doi.org/10.1371/journal.pone.0146756 -->

<!-- -------------- -->

<!--chapter:end:index.Rmd-->

# Data and Code Access {#erddap}

### About

The Technical Documentation for the State of the Ecosystem reports is a [bookdown](https://bookdown.org) document; hosted on the NOAA Northeast Fisheries Science Center Ecosystems Dynamics and Assessment Branch [Github page](https://github.com/NOAA-EDAB), and developed in R. Derived data used to populate figures in this document are queried directly from the [ecodata](https://github.com/NOAA-EDAB/ecodata) R package or the NEFSC [ERDDAP server](https://comet.nefsc.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000). ERDDAP queries are made using the R package [rerddap](https://cran.r-project.org/web/packages/rerddap/vignettes/Using_rerddap.html).  

### Accessing source data and build code

In this technical documentation, we hope to shine a light on the processing and analytical steps involved to get from source data to final product. This means that whenever possible, we have included the code involved in source data extraction, processing, and analyses. We have also attempted to thoroughly describe all methods in place of or in supplement to provided code. Example plotting code for each indicator is presented in sections titled "Plotting", and these code chunks can be used to recreate the figures presented in State of the Ecosystem reports[^1].

Source data for the derived indicators in this document are linked to in the text unless there are privacy concerns involved. In that case, it may be possible to access source data by reaching out to the Point of Contact associated with that data set. Derived data sets make up the majority of the indicators present in the State of the Ecosystem reports. These data sets are available for download through the [ecodata](https://github.com/NOAA-EDAB/ecodata) R package and NEFSC [ERDDAP server](https://comet.nefsc.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000). The script [get_erddap.R](https://github.com/NOAA-EDAB/tech-doc/blob/master/R/get_erddap.R) can be used to query individual data sets from ERDDAP. An example of its usage can be found [here](#stockstatus).

### Building the document

Start a local build of the SOE bookdown document by first cloning the project's associated [git repository](https://github.com/NOAA-EDAB/tech-doc). Next, if you would like to build a past version of the document, use `git checkout [version_commit_hash]` to revert the project to a past commit of interest, and set `build_latest <- FALSE` in the following code chunk. This will ensure the project builds from a cached data set, and not the most updated versions present on the NEFSC ERDDAP server. Once the `tech-doc.Rproj` file is opened in RStudio, run `bookdown::serve_book()` from the console to build the document. 


```{r query, echo = T, eval = F, message=F, warning=F}

build_latest <- FALSE

if (build_latest){
  
  # Relative working directories
  data.dir  <- here::here('data')
  r.dir <- here::here('R')
  
  #Source function for querying ERDDAP server
  source(file.path(r.dir,"get_erddap.R"))
  
  #Set URL for COMET (server where NEFSC ERDDAP lives)
  comet <- 'https://comet.nefsc.noaa.gov/erddap/'
  
  #List datasets on the NEFSC ERDDAP
  tab_list <- ed_datasets(url = comet)
  
  #Get updated data set IDs
  erddap_datasets <- tab_list %>% 
    filter(str_detect(Dataset.ID, "soe_v")) %>% 
    get_erddap(id = NULL)
  
  #Save and clean updated IDs for use in rest of report
  save(erddap_datasets, file = file.path(data.dir, "ERDDAP_datasets.Rdata"))
  
  # Exclude stock assessment status data, which have unique structure
  erddap_datasets <- erddap_datasets %>%
    dplyr::filter(!str_detect(Dataset.ID, "assess")) 
  
  #Create SOE parent data set, filter out NAs. This queries based on 
  #data set IDs that were collected above
  SOE.data.erd <- sprintf("http://comet.nefsc.noaa.gov/erddap/tabledap/%s.csv",
                           erddap_datasets$Dataset.ID) %>% 
    purrr::map(function(x) {
      readr::read_csv(url(x))
    }) %>% 
    do.call(rbind,.) %>% 
    mutate(Value = as.numeric(Value)) %>% 
      dplyr::filter(!is.na(Value))
  
  #Convert to data.table
  SOE.data <- as.data.table(SOE.data.erd)
  
  #Save data
  save(SOE.data, file = file.path(data.dir,"SOE_data_erddap.Rdata"))
}
```

#### A note on data structures

The majority of the derived time series used in State of the Ecosystem reports are in long format. This approach was taken so that all disparate data sets could be "bound" together for ease of use in our base plotting [functions]((https://github.com/NOAA-EDAB/tech-doc/blob/master/R/BasePlot_source.R)).

[^1]: There are multiple R scripts sourced throughout this document in an attempt to keep code concise. These scripts include [BasePlot_source.R](https://github.com/NOAA-EDAB/tech-doc/blob/master/R/BasePlot_source.R), [GIS_source.R](https://github.com/NOAA-EDAB/tech-doc/blob/master/R/GIS_source.R), and [get_erddap.R](https://github.com/NOAA-EDAB/tech-doc/blob/master/R/get_erddap.R). The scripts `BasePlot_source.R` and `GIS_source.R` refer to deprecated code used prior to the 2019 State of the Ecosystem reports. Indicators that were not included in reports after 2018 make use of this syntax, whereas newer indicators typically use `ggplot2` for plotting.


<!--chapter:end:chapters/erddap_query_and_build.Rmd-->

# Aggregate Groups {#aggroups}

**Description**: Mappings of species into aggregate group categories for different analyses

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) 

**Indicator category**: Synthesis of published information 

**Contributor(s)**: Geret DePiper, Sarah Gaichas, Sean Hardison, Sean Lucey
  
**Data steward**: Sean Lucey <Sean.Lucey@noaa.gov>
  
**Point of contact**: Sean Lucey <Sean.Lucey@noaa.gov>
  
**Public availability statement**: Source data is available to the public (see Data Sources). 


## Methods
The State of the Ecosystem (SOE) reports are delivered to the New England Fishery Management Council (NEFMC) and Mid-Atlantic Fishery Management Council (MAFMC) to provide ecosystems context.  To better understand that broader ecosystem context, many of the indicators are reported at an aggregate level rather than at a single species level.  Species were assigned to an aggregate group following the classification scheme of @garrison2000dietary and @link2006EMAX.  Both works classified species into feeding guilds based on food habits data collected at the Northeast Fisheries Science Center (NEFSC).  In 2017, the SOE used seven specific feeding guilds (plus an "other" category; Table \@ref(tab:soe2017class)).  These seven were the same guilds used in @garrison2000dietary, which also distinguished ontogentic shifts in species diets.  

For the purposes of the SOE, species were only assigned to one category based on the most prevalent size available to commercial fisheries.  However, several of those categories were confusing to the management councils, so in 2018 those categories were simplified to five (plus "other"; Table \@ref(tab:soe2018class)) along the lines of @link2006EMAX.  In addition to feeding guilds, species managed by the councils have been identified.  This is done to show the breadth of what a given council is responsible for within the broader ecosystem context.

```{r soe2017class, eval = T, echo = F}
soe.17.class <- data.frame('Feeding Guild' = c('Apex Predator', 'Piscivore', 
                                               'Macrozoo-piscivore', 'Macroplanktivore', 
                                               'Mesoplanktivore', 'Benthivore',
                                               'Benthos', 'Other'),
                           Description = c('Top of the food chain', 'Fish eaters', 
                                           'Shrimp and small fish eaters', 'Amphipod and shrimp eaters',
                                           'Zooplankton eaters', 'Bottom eaters',
                                           'Things that live on the bottom', 
                                           'Things not classified above'))

kable(soe.17.class, booktabs = TRUE,
      caption = "Aggregate groups use in 2017 SOE.  Classifications are based on Garrison and Link 2000. \\label{}")
```

```{r soe2018class, eval = T, echo = F}
soe.18.class <- data.frame('Feeding Guild' = c('Apex Predator', 'Piscivore', 
                                               'Planktivore', 'Benthivore',
                                               'Benthos', 'Other'),
                           Description = c('Top of the food chain', 'Fish eaters', 
                                           'Zooplankton eaters', 'Bottom eaters',
                                           'Things that live on the bottom', 
                                           'Things not classified above'))

kable(soe.18.class, booktabs = TRUE,
      caption = "Aggregate groups use in 2018 SOE.  Classifications are based on Link et al. 2006.")
```

### Data sources

In order to match aggregate groups with various data sources, a look-up table was generated which includes species' common names (COMNAME) along with their scientific names (SCINAME) and several species codes. SVSPP codes are used by the NEFSC Ecosystems Surveys Branch in their fishery-independent Survey Database (SVDBS), while NESPP3 codes refer to the codes used by the Commercial Fisheries Database System (CFDBS) for fishery-dependent data. A third species code provided is the ITISSPP, which refers to species identifiers used by the Integrated Taxonomic Information System (ITIS). Digits within ITIS codes are hierarchical, with different positions in the identifier referring to higher or lower taxonomic levels. More information about the SVDBS, CFDBS, and ITIS species codes are available in the links provided below.

Management responsibilities for different species are listed under the column "Fed.managed" (NEFMC, MAFMC, or JOINT for jointly managed species). More information about these species is available on the FMC websites listed below. Species groupings listed in the "NEIEA" column were developed for presentation on the Northeast Integrated Ecosystem Assessment (NE-IEA) website. These groupings are based on EMAX groupings [@link2006EMAX], but were adjusted based on conceptual models developed for the NE-IEA program that highlight focal components in the NE-LME (i.e. those components with the largest potential for perturbing ecosystem dynamics). NE-IEA groupings were further simplified to allow for effective communication through the NE-IEA website.

#### Supplemental information

See the following links for more information regarding the NEFSC ESB Bottom Trawl Survey, CFDBS, and ITIS:  

*    https://www.itis.gov/  
*    https://inport.nmfs.noaa.gov/inport/item/22561  
*    https://inport.nmfs.noaa.gov/inport/item/22560  
*    https://inport.nmfs.noaa.gov/inport/item/27401  	

More information about the NE-IEA program is available [here](http://integratedecosystemassessment.noaa.gov).

More information about the New Engalnd Fisheries Management Council is available [here](https://www.nefmc.org/).

More information about the Mid-Atlantic Fisheries Management Council is available [here](http://www.mafmc.org/).

### Data extraction 

Species lists are pulled from SVDBS and CFDBS.  They are merged using the ITIS code.  Classifications from Garrison and Link [@garrison2000dietary] and Link et al. [@link2006EMAX] are added manually. The R code used in the extraction process is presented below.


```{r, echo = T, eval = F}
#Species list
if(Sys.info()['sysname']=="Windows"){
  data.dir <- "L:\\EcoAP\\Data\\survey"
  out.dir  <- "L:\\EcoAP\\Data\\survey"
  memory.limit(4000)
}
if(Sys.info()['sysname']=="Linux"){
  data.dir  <- "/home/slucey/slucey/EcoAP/Data/survey"
  out.dir   <- "/home/slucey/slucey/EcoAP/Data/survey"
  out.dir.2 <- "/home/slucey/slucey/EcoAP/Data/Commercial"
  uid       <- 'slucey'
  cat("Oracle Password: ")
  pwd <- scan(stdin(), character(), n = 1)
}

library(RODBC); library(data.table)

if(Sys.info()['sysname']=="Windows"){
  channel <- odbcDriverConnect()
} else {
  channel <- odbcConnect('sole', uid, pwd)
}

#Grab svspp code by itis code
svspp <- as.data.table(sqlQuery(channel, "select SVSPP, ITISSPP, COMNAME, SCINAME 
                                from ITIS_Lookup"))

#Grab cfspp by itis code
cfspp <- as.data.table(sqlQuery(channel, "select NESPP4, SPECIES_ITIS, 
                                COMMON_NAME, SCIENTIFIC_NAME 
                                from CFDBS.Species_itis_ne"))
setnames(cfspp, 'SPECIES_ITIS', 'ITISSPP')
cfspp[, NESPP3 := as.numeric(substr(sprintf('%04d', NESPP4), 1, 3))]
setkey(cfspp, NESPP3)
cfspp <- unique(cfspp)
cfspp[, NESPP4 := NULL]

setkey(cfspp, ITISSPP, NESPP3)
cfspp <- unique(cfspp, by = key(cfspp))

#Merge to master species list
spp <- merge(svspp, cfspp, by = 'ITISSPP', all = T)
spp <- spp[!(is.na(SVSPP) & is.na(NESPP3)), ]

#Fix known issues
spp <- spp[!SVSPP %in% c(193, 310), ]

spp[ITISSPP == 630979, SVSPP  := 193] #Ocean Pout
spp[ITISSPP == 620992, SVSPP  := 310] #Deepsea red crab
spp[ITISSPP == 172783, SVSPP  := 104] #Fourspot flounder
spp[ITISSPP == 166283, SVSPP  := 112] #John Dory
spp[ITISSPP == 161731, SVSPP  :=  36] #Menhaden
spp[ITISSPP ==  98671, SVSPP  := 311] #Cancer Crabs unk
spp[ITISSPP ==  98455, SVSPP  := 317] #Spider crabs
spp[ITISSPP == 159772, NESPP3 := 150] #Hagfish
spp[ITISSPP == 166284, NESPP3 := 188] #John Dory
spp[ITISSPP == 98670,  NESPP3 := 714] #Cancer Crabs unk

spp[ITISSPP %in% c(630979, 620992), COMNAME := COMMON_NAME]
spp[ITISSPP %in% c(630979, 620992), SCINAME := SCIENTIFIC_NAME]

#Drop extra columns
spp[is.na(COMNAME), COMNAME := COMMON_NAME]
spp[is.na(SCINAME), SCINAME := SCIENTIFIC_NAME]
spp[, c('COMMON_NAME', 'SCIENTIFIC_NAME') := NULL]
setcolorder(spp, c('ITISSPP', 'SVSPP', 'NESPP3', 'COMNAME', 'SCINAME'))
setkey(spp, SVSPP, NESPP3)

#-------------------------------------------------------------------------------
#Add management authority
mafmc <- c(103, 121, 131, 135, 141, 143, 151, 403, 409, 502, 503, 621)
nefmc <- c(22:28, 32, 69, 72:77, 101, 102, 105:107, 155, 193, 310, 401, 894)
joint <- c(15, 197)
spp[SVSPP %in% mafmc, MANAGE := 'MAFMC']
spp[SVSPP %in% nefmc, MANAGE := 'NEFMC']
spp[SVSPP %in% joint, MANAGE := 'JOINT']

#-------------------------------------------------------------------------------
#Add functional groups
#From NEFMC EBFM PDT work
#See Functional_group_table_Mike.csv in slucey/EcoAP/EBFM_PDT
spp[, EBFM.PDT := factor(NA, levels = c('Apex Predator', 'Benthivore', 'Benthos',
                                        'Macroplanktivore', 'Macrozoo-piscivore',
                                        'Mesoplanktivore', 'Piscivore', 'Other'))]
spp[SVSPP %in% c(11, 700, 704, 747), EBFM.PDT := 'Apex Predator']
spp[SVSPP %in% c(14, 22, 25, 74, 102, 105, 106, 107, 141, 143, 151, 164, 172,
                   176, 177, 192, 193, 301, 310, 312, 314, 317, 322, 501),
    EBFM.PDT := 'Benthivore']
spp[SVSPP %in% c(331, 336, 401, 403, 409), EBFM.PDT := 'Benthos']
spp[NESPP3 %in% c(775, 781, 805, 806), EBFM.PDT := 'Benthos']  
spp[SVSPP %in% c(76, 163, 168, 171, 502, 503), EBFM.PDT := 'Macroplanktivore']
spp[SVSPP %in% c(13, 24, 26, 27, 75, 77, 84, 108, 112, 155, 156, 311),
    EBFM.PDT := 'Macrozoo-piscivore']
spp[SVSPP %in% c(32, 33, 34, 35, 36, 121, 131), EBFM.PDT := 'Mesoplanktivore']
spp[SVSPP %in% c(15, 23, 28, 69, 72, 73, 101, 103, 104, 135, 139, 145, 197),
    EBFM.PDT := 'Piscivore']
spp[is.na(EBFM.PDT), EBFM.PDT := 'Other']

#Fix known issues
spp[SVSPP == 160, EBFM.PDT := 'Macroplanktivore']

#Add EMAX
load(file.path(data.dir, 'EMAX_groups.RData'))
emax <- emax[!is.na(SVSPP), ]
spp <- merge(spp, emax[, list(SVSPP, EMAX, Fall.q, Spring.q)], by = 'SVSPP', all.x = T)

#reduce rows
setkey(spp, SVSPP, NESPP3)
species <- unique(spp, by = key(spp))

#Remove EMAX q's
species[, c('Fall.q', 'Spring.q') := NULL]

#Expand EMAX abbreviations
species[EMAX == 'BG', EMAX := 'Benthivore Gadiformes']
species[EMAX == 'BE', EMAX := 'Benthivore Elasmobranchs']
species[EMAX == 'BP', EMAX := 'Benthivore Pleuronectiformes']
species[EMAX == 'BS', EMAX := 'Benthivore Scorpaeniformes']
species[EMAX == 'BO', EMAX := 'Benthivore Others']
species[EMAX == 'BC', EMAX := 'Benthivore Perciformes']
species[EMAX == 'PG', EMAX := 'Piscivore Gadiformes']
species[EMAX == 'PE', EMAX := 'Piscivore Elasmobranchs']
species[EMAX == 'PO', EMAX := 'Piscivore Others']
species[EMAX == 'OE', EMAX := 'Omnivore Elasmobranchs']
species[EMAX == 'OO', EMAX := 'Omnivore Others']
species[EMAX == 'SC', EMAX := 'Southern Perciformes']
species[EMAX == 'SP', EMAX := 'Southern Pleuronectiformes']
species[EMAX == 'SE', EMAX := 'Southern Elasmobranchs']
species[EMAX == 'SG', EMAX := 'Southern Gadiformes']
species[EMAX == 'SO', EMAX := 'Southern Others']

#Rename columns
setnames(species, c('MANAGE', 'EBFM.PDT', 'Feeding.guild'), c('Fed.Managed', 'SOE.17', 'SOE.18'))

#Add classification from Garrison/Link 2000
size <- data.table(COMNAME = c(rep('SMOOTH DOGFISH',           2), 
                               rep('SPINY DOGFISH',            3),
                               rep('WINTER SKATE',             4),
                               rep('LITTLE SKATE',             2),
                               rep('SMOOTH SKATE',             1),
                               rep('THORNY SKATE',             4),
                               rep('ATLANTIC HERRING',         2),
                               rep('ALEWIFE',                  1),
                               rep('SILVER HAKE',              3),
                               rep('ATLANTIC COD',             4),
                               rep('HADDOCK',                  3),
                               rep('POLLOCK',                  4),
                               rep('WHITE HAKE',               3),
                               rep('RED HAKE',                 3),
                               rep('SPOTTED HAKE',             2),
                               rep('AMERICAN PLAICE',          3), 
                               rep('SUMMER FLOUNDER',          2),
                               rep('FOURSPOT FLOUNDER',        2),
                               rep('YELLOWTAIL FLOUNDER',      3),
                               rep('WINTER FLOUNDER',          3),
                               rep('WITCH FLOUNDER',           2), 
                               rep('WINDOWPANE',               2),
                               rep('GULF STREAM FLOUNDER',     1), 
                               rep('ATLANTIC MACKEREL',        3),
                               rep('BUTTERFISH',               1),
                               rep('BLUEFISH',                 3),
                               rep('ATLANTIC CROAKER',         2),
                               rep('BLACK SEA BASS',           2), 
                               rep('SCUP',                     1),
                               rep('WEAKFISH',                 2),
                               rep('ACADIAN REDFISH',          1),
                               rep('LONGHORN SCULPIN',         2),
                               rep('SEA RAVEN',                2),
                               rep('NORTHERN SAND LANCE',      1),
                               rep('OCEAN POUT',               1),
                               rep('FAWN CUSK-EEL',            1),
                               rep('GOOSEFISH',                1),
                               rep('ATLANTIC SHARPNOSE SHARK', 1), 
                               rep('NORTHERN SHORTFIN SQUID',  1),
                               rep('LONGFIN SQUID',            1)),
                   SizeCat = c('M', 'L', 'S', 'M', 'L', 'S', 'M', 'L', 'XL', 'S',
                               'M', 'M', 'S', 'M', 'L', 'XL', 'S','M','M', 'S',
                               'M', 'L', 'S', 'M', 'L', 'XL', 'S', 'M', 'L', 'S',
                               'M', 'L', 'XL', 'S', 'M', 'L', 'S', 'M', 'L', 'S',
                               'M',  'S', 'M', 'L', 'M', 'L', 'S', 'M', 'S', 'M',
                               'L', 'S', 'M', 'L', 'M', 'L', 'S', 'M', 'S', 'S',
                               'M', 'L', 'S', 'S', 'M', 'L', 'S', 'M', 'S', 'M',
                               'M', 'S', 'M', 'M', 'S', 'M', 'S', 'M', 'M', 'L',
                               'L', 'L', 'L', 'L', 'L'),
                   Min.size = c(41, 61, 10, 41, 61, 10, 31, 61, 81, 10, 31, 31,
                                10, 31, 61, 81, 10, 21, 21, 10, 21, 41, 10, 21, 
                                51, 81, 10, 21, 51, 10, 21, 51, 81, 10, 21, 41, 10, 21, 41, 10, 21, 
                                10, 21, 41, 21, 41, 10, 21, 10, 21, 41, 10, 21, 
                                41, 21, 41, 10, 21, 10, 10, 21, 36, 10, 10, 31,
                                71, 10, 26, 10, 26, 26, 10, 26, 26, 10, 26, 10, 
                                26, 11, 61, 61, 61, 61, 31, 31),
                   Garrison.Link = as.character(c(1, 1, 2, 2, 6, 3, 3, 6, 6, 3, 3, 4, 5, 5, 6, 
                                     6, 2, 2, 2, 4, 4, 6, 3, 3, 6, 6, 5, 5, 5, 4, 
                                     4, 4, 4, 3, 4, 6, 3, 3, 4, 3, 6, 5, 5, 5, 6,
                                     6, 3, 6, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3, 5, 2,
                                     2, 2, 2, 6, 6, 6, 5, 5, 1, 1, 5, 6, 6, 4, 3, 
                                     3, 6, 6, 2, 5, 3, 6, 6, 2, 2))
                                )

#Give Garrison.link guilds meaniful names
size[Garrison.Link == 1, Garrison.Link := 'Crab Eaters']
size[Garrison.Link == 2, Garrison.Link := 'Planktivores']
size[Garrison.Link == 3, Garrison.Link := 'Amphipod/Shrimp Eaters']
size[Garrison.Link == 4, Garrison.Link := 'Shrimp/Small Fish Eaters']
size[Garrison.Link == 5, Garrison.Link := 'Benthivores']
size[Garrison.Link == 6, Garrison.Link := 'Piscivores']

#Merge Garrison Link into species list
species <- merge(species, size, by = 'COMNAME', all = T)

#Set column order
setcolorder(species, c('ITISSPP', 'SVSPP', 'NESPP3', 'COMNAME', 'SCINAME', 
                       'SizeCat', 'Min.size', 'Fed.Managed', 'Garrison.Link',
                       'SOE.17', 'SOE.18', 'EMAX'))

#Add NE.IEA.web category
#get rerddap
devtools::install_github("ropensci/rerddap")
library(rerddap) 

#Set URL for COMET (server where NEFSC ERDDAP lives)
comet <- 'https://comet.nefsc.noaa.gov/erddap/'

#List datasets on the NEFSC ERDDAP
tab_list <- ed_datasets(url = comet)

#download a tabular dataset (input is dataset id; this works on vectors)
ed_spp <- as.data.table(sprintf("https://comet.nefsc.noaa.gov/erddap/tabledap/%s.csv", "species_groups_2018") %>% 
  
  purrr::map(function(x) {
    
    readr::read_csv(url(x))
    
  }))

neiea <- unique(ed_spp[!is.na(NEIEA), list(COMNAME, NEIEA)])

species <- merge(species, neiea, by = 'COMNAME', all = T)

save(species, file = file.path(out.dir, 'SOE_species_list.RData'))
```








<!--chapter:end:chapters/aggregate_groups.Rmd-->

# Annual SST Cycles

**Description**: Annual SST Cycles

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018), State of the Ecosystem - Mid-Atlantic (2018) 

**Indicator category**: Database pull with analysis

**Contributor(s)**: Sean Hardison, Vincent Saba
  
**Data steward**: Sean Hardison, <sean.hardison@noaa.gov>
  
**Point of contact**: Sean Hardison, <sean.hardison@noaa.gov>
  
**Public availability statement**: Source data are available [here](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html). 

## Methods

### Data sources
Data for annual sea surface tempature cycles were derived from the NOAA optimum interpolation sea surface temperature high resolution dataset ([NOAA OISST V2 dataset](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html)) provided by NOAA/OAR/ESRL PSD, Boulder, CO. The data extend from 1981 to present, and provide a 0.25&deg; x 0.25&deg; global grid of SST measurements [@Reynolds2007]. Gridded SST data were masked according to the extent of Ecological Production Units in the Northeast Large Marine Ecosystem (NE-LME) (See ["EPU_Extended" shapefiles](https://github.com/NOAA-EDAB/tech-doc/tree/master/gis)).


### Data extraction 
Daily mean sea surface temperature data for 2017 and for each year during the period of 1981-2012 were downloaded from the NOAA [OI SST V2 site](https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.highres.html) to derive the long-term climatological mean for the period. The use of a 30-year climatological reference period is a standard procedure for metereological observing [@WMO2017]. These reference periods serve as benchmarks for comparing current or recent observations, and for the development of standard anomaly data sets. The reference period of 1982-2012 was chosen to be consistent with previous versions of the State of the Ecosystem report. 

R code used in extraction and processing:
```{r, echo = T, eval = F}

#libraries
library(ncdf4);library(dplyr)
library(readr);library(tidyr)
library(sp);library(rgdal)
library(raster);library(stringr)

#get spatial polygons for Ecological Production Units (EPUs) that are used to clip SST data.
EPU <- readOGR('Extended_EPU')

map.crs <- CRS("+proj=longlat +lat_1=35 +lat_2=45 +lat_0=40 +lon_0=-77 +x_0=0
               +y_0=0 +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")

#find long term daily mean SSTs and 2017 SST anomaly
MAB_sst_daily_mean <- NULL
GB_sst_daily_mean <- NULL
GOM_sst_daily_mean <- NULL

#I split the data into three directories to loop through in separate R sessions concurrently. 


for (dir. in 1:3){
  
  #Loop through directories
  setwd(paste0('c:/users/sean.hardison/documents/sst_data/',dir.))
  print(getwd())
  
  for (f in 1:length(list.files())){
    
    if (!str_detect(list.files()[f],".nc")){
      print(paste(list.files()[f],"is not a raster")) #Based on file type
      next
    }
    
    for (j in c("MAB","GB","GOM")){
      
      sub_region <- EPU[EPU@data$EPU == j,]
      y <- as.numeric(str_extract(list.files()[f],"[0-9]+")) #get year
      
      for (i in 1:365){
        print(paste(j,y,i))
        daily_mean <- raster(paste0(list.files()[f]), band = i) #get band
        
        #set crs
        daily_mean@crs <- sub_region@proj4string 
        
        
        #rotate to lon scale from 0-360 to -180-180
        daily_mean <- rotate(daily_mean)
        
        #mask raster with spatialpolygon
        daily_mean_clipped <- mask(daily_mean, sub_region)
        
        
        #add mean value to data.frame
        assign(paste0(j,"_sst_daily_mean"),rbind(get(paste0(j,"_sst_daily_mean")),
                                                 c(mean(daily_mean_clipped@data@values, na.rm = T),y,i)))
        
      }
    }
    
    
  }
}

#Put results into data.frames
mab <- data.frame(EPU = "MAB",
                  year = MAB_sst_daily_mean[,2],
                  day = MAB_sst_daily_mean[,3],
                  Value = MAB_sst_daily_mean[,1])

gb <- data.frame(EPU = "GB",
                 year = GB_sst_daily_mean[,2],
                 day = GB_sst_daily_mean[,3],
                 Value = GB_sst_daily_mean[,1])

gom <- data.frame(EPU = "GOM",
                  year = GOM_sst_daily_mean[,2],
                  day = GOM_sst_daily_mean[,3],
                  Value = GOM_sst_daily_mean[,1])


data3 <- rbind(mab, gb, gom)

#Save as 1 of 3 files (one for each directory containing daily mean data)
save(data3, file = "dir3_sst.Rdata")

#--------------------------2017 SSTs----------------------------#
MAB_2017 <- NULL
GB_2017 <- NULL
GOM_2017 <- NULL

for (j in c("MAB","GB","GOM")){
  
  sub_region <- EPU[EPU@data$EPU == j,]
  
  for (i in 1:365){
    print(paste(j,i))
    daily_mean <- raster("sst.day.mean.2017.nc", band = i) #get band
    
    
    #set crs
    daily_mean@crs <- sub_region@proj4string 
    
    
    #rotate to lon scale from 0-360 to -180-180
    daily_mean <- rotate(daily_mean)
    
    #mask raster with spatialpolygon
    daily_mean_clipped <- mask(daily_mean, sub_region)
    
    
    #add mean value to data.frame
    assign(paste0(j,"_2017"),rbind(get(paste0(j,"_2017")),
                                             c(mean(daily_mean_clipped@data@values, na.rm = T),"2017",i)))
    
  }
}
#Put results into data.frames
mab_2017 <- data.frame(EPU = "MAB",
                  year = MAB_2017[,2],
                  day = MAB_2017[,3],
                  Value = MAB_2017[,1])

gb_2017 <- data.frame(EPU = "GB",
                 year = GB_2017[,2],
                 day = GB_2017[,3],
                 Value = GB_2017[,1])

gom_2017 <- data.frame(EPU = "GOM",
                  year = GOM_2017[,2],
                  day = GOM_2017[,3],
                  Value = GOM_2017[,1])


#Final 2017 daily mean data
sst_2017 <- rbind(mab_2017, gb_2017, gom_2017)
#save(sst_2017, file = "sst_2017.Rdata")

```

### Data analysis 
We calculated the long-term mean and standard deviation of SST over the period of 1982-2012 for each EPU, as well as the daily mean for 2017.  

```{r analyses, echo = T, eval = F}
#----------------------Load results--------------------------#
load("dir1_sst.Rdata")
load("dir2_sst.Rdata")
load("dir3_sst.Rdata")
load("sst_2017.Rdata")

#Get long term mean and standard deviation
d <- rbind(data1, data2, data3)

ltm <- d %>% group_by(EPU, day) %>% dplyr::summarise(mean  = mean(Value),
                                                     sd = sd(Value)) 

```

### Plotting
```{r plotting, echo = T, eval = T, fig.cap = "Long-term mean SSTs for the Mid-Atlantic Bight (A), Georges Bank (B), and Gulf of Maine (C). Orange and cyan shading show where the 2017 daily SST values were above or below the long-term mean respectively; red and dark blue shades indicate days when the 2017 mean exceeded +/- 1 standard deviation from the long-term mean.", fig.width=8, fig.height=3.25, fig.align='center'}


# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

##---------------------------------MAB-----------------------------------------#
par(mfrow = c(1,3))
doy <- as.numeric(SOE.data[SOE.data$Var == "sst mean 2017 MAB",]$Time)
val_2017 <- SOE.data[SOE.data$Var == "sst mean 2017 MAB",]$Value
val_LT <- SOE.data[SOE.data$Var == "sst mean long term MAB",]$Value
val_LT_sd <- SOE.data[SOE.data$Var == "sst sd long term MAB",]$Value


# val_2017 <- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT <- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT_sd <- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
doy <- seq(doy[1],doy[length(doy)],length.out = 365*1)


above_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT[i]){
    above_mean[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT [i]){
    above_mean[i] <- NA
  }
}

below_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i]){
    below_mean[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT [i]){
    below_mean[i] <- NA
  }
}

above_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT_sd[i] + val_LT[i]){
    above_sd[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT_sd [i] + val_LT[i]){
    above_sd[i] <- NA
  }
}

below_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i] - val_LT_sd[i]){
    below_sd[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT[i] - val_LT_sd [i]){
    below_sd[i] <- NA
  }
}

#Lines for polygons
above_sd[is.na(above_sd)] <- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))]
below_sd[is.na(above_sd)] <- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] 
above_mean[is.na(above_mean)] <- val_LT[which(is.na(above_mean))]
below_mean[is.na(below_mean)] <- val_LT[which(is.na(below_mean))]

upper <- val_LT_sd + val_LT
lower <- val_LT - val_LT_sd

#Null figure
plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,25), las = 1, 
     ylab = "", yaxt = "n", xaxt = "n", xlab = "")
axis(2, cex.axis = 1.25, las = 1)
axis(1,  labels = c("Jan","Mar","May","July","Sep","Nov","Jan"), 
     at = c(1,61,122,183,245,306,365), cex.axis= 1.25)
mtext(2, line = 2.3, text = expression(paste("Mean SST (",degree,"C)")), cex = 1.1)
mtext(1, line = 2.5, text = "Time", cex = 1.1)
text(15,25*.95,"A",cex = 1.5)
# +/- 1 sd
polygon(c(doy, rev(doy)),
        c(upper, rev(lower)),
        col = "grey85", border = NA)

#Fills plot
polygon(c(doy, rev(doy)),
        c(below_mean + (val_LT-below_mean), rev(below_mean)),
        col = "lightblue", border = NA)
polygon(c(doy, rev(doy)),
        c(above_mean - (above_mean-val_LT), rev(above_mean)),
        col = "orange", border = NA)
polygon(c(doy, rev(doy)),
        c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)),
        col = "red", border = NA)
polygon(c(doy, rev(doy)),
        c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)),
        col = "blue", border = NA)
points(doy,val_LT, type = "l", lwd = 1, "grey90")


##-------------------------------------GB-------------------------------------#

doy <- as.numeric(SOE.data[SOE.data$Var == "sst mean 2017 GB",]$Time)
val_2017 <- SOE.data[SOE.data$Var == "sst mean 2017 GB",]$Value
val_LT <- SOE.data[SOE.data$Var == "sst mean long term GB",]$Value
val_LT_sd <- SOE.data[SOE.data$Var == "sst sd long term GB",]$Value


# val_2017 <- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT <- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT_sd <- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
doy <- seq(doy[1],doy[length(doy)],length.out = 365*1)


above_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT[i]){
    above_mean[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT [i]){
    above_mean[i] <- NA
  }
}

below_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i]){
    below_mean[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT [i]){
    below_mean[i] <- NA
  }
}

above_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT_sd[i] + val_LT[i]){
    above_sd[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT_sd [i] + val_LT[i]){
    above_sd[i] <- NA
  }
}

below_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i] - val_LT_sd[i]){
    below_sd[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT[i] - val_LT_sd [i]){
    below_sd[i] <- NA
  }
}

#Lines for polygons
above_sd[is.na(above_sd)] <- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))]
below_sd[is.na(above_sd)] <- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] 
above_mean[is.na(above_mean)] <- val_LT[which(is.na(above_mean))]
below_mean[is.na(below_mean)] <- val_LT[which(is.na(below_mean))]

upper <- val_LT_sd + val_LT
lower <- val_LT - val_LT_sd

#Null figure
plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,21), las = 1, 
     ylab = "", yaxt = "n", xaxt = "n", xlab = "")
axis(2, cex.axis = 1.25, las = 1)
axis(1,  labels = c("Jan","Mar","May","July","Sep","Nov","Jan"), 
     at = c(1,61,122,183,245,306,365), cex.axis= 1.25)
#mtext(2, line = 2.5, text = expression(paste("Mean SST (",degree,"C)")), cex = 1.1)
mtext(1, line = 2.5, text = "Time", cex = 1.1)
text(15,21*.95,"B",cex = 1.5)
# +/- 1 sd
polygon(c(doy, rev(doy)),
        c(upper, rev(lower)),
        col = "grey85", border = NA)

#Fills plot
polygon(c(doy, rev(doy)),
        c(below_mean + (val_LT-below_mean), rev(below_mean)),
        col = "lightblue", border = NA)
polygon(c(doy, rev(doy)),
        c(above_mean - (above_mean-val_LT), rev(above_mean)),
        col = "orange", border = NA)
polygon(c(doy, rev(doy)),
        c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)),
        col = "red", border = NA)
polygon(c(doy, rev(doy)),
        c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)),
        col = "blue", border = NA)
points(doy,val_LT, type = "l", lwd = 1, "grey90")

#----------------------------------------------------------------------------#

## SST GOM

doy <- as.numeric(SOE.data[SOE.data$Var == "sst mean 2017 GOM",]$Time)
val_2017 <- SOE.data[SOE.data$Var == "sst mean 2017 GOM",]$Value
val_LT <- SOE.data[SOE.data$Var == "sst mean long term GOM",]$Value
val_LT_sd <- SOE.data[SOE.data$Var == "sst sd long term GOM",]$Value


# val_2017 <- approx(doy,val_2017, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT <- approx(doy,val_LT, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
# val_LT_sd <- approx(doy,val_LT_sd, xout = seq(doy[1],doy[length(doy)],length.out = 365*1))$y
doy <- seq(doy[1],doy[length(doy)],length.out = 365*1)


above_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT[i]){
    above_mean[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT [i]){
    above_mean[i] <- NA
  }
}

below_mean <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i]){
    below_mean[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT [i]){
    below_mean[i] <- NA
  }
}

above_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] >= val_LT_sd[i] + val_LT[i]){
    above_sd[i] <- val_2017[i]
  } else if (val_2017[i] < val_LT_sd [i] + val_LT[i]){
    above_sd[i] <- NA
  }
}

below_sd <- NULL
for (i in 1:length(val_2017)){
  if (val_2017[i] <= val_LT[i] - val_LT_sd[i]){
    below_sd[i] <- val_2017[i]
  } else if (val_2017[i] > val_LT[i] - val_LT_sd [i]){
    below_sd[i] <- NA
  }
}

#Lines for polygons
above_sd[is.na(above_sd)] <- val_LT_sd[which(is.na(above_sd))] + val_LT[which(is.na(above_sd))]
below_sd[is.na(above_sd)] <- val_LT[which(is.na(below_sd))] - val_LT_sd[which(is.na(below_sd))] 
above_mean[is.na(above_mean)] <- val_LT[which(is.na(above_mean))]
below_mean[is.na(below_mean)] <- val_LT[which(is.na(below_mean))]

upper <- val_LT_sd + val_LT
lower <- val_LT - val_LT_sd

#Null figure
plot(NULL, xlim = c(doy[1],doy[(length(doy))]), ylim = c(4,21), las = 1, 
     ylab = "", yaxt = "n", xaxt = "n", xlab = "")
axis(2, cex.axis = 1.25, las = 1)
axis(1,  labels = c("Jan","Mar","May","July","Sep","Nov","Jan"), 
     at = c(1,61,122,183,245,306,365), cex.axis= 1.25)
#(2, line = 2.5, text = expression(paste("Mean SST (",degree,"C)")), cex = 1.1)
mtext(1, line = 2.5, text = "Time", cex = 1.1)
text(15,21*.95,"C",cex = 1.5)
# +/- 1 sd
polygon(c(doy, rev(doy)),
        c(upper, rev(lower)),
        col = "grey85", border = NA)

#Fills plot
polygon(c(doy, rev(doy)),
        c(below_mean + (val_LT-below_mean), rev(below_mean)),
        col = "lightblue", border = NA)
polygon(c(doy, rev(doy)),
        c(above_mean - (above_mean-val_LT), rev(above_mean)),
        col = "orange", border = NA)
polygon(c(doy, rev(doy)),
        c(above_sd - (above_sd-(val_LT + val_LT_sd)), rev(above_sd)),
        col = "red", border = NA)
polygon(c(doy, rev(doy)),
        c(below_sd + (below_sd-(val_LT - val_LT_sd)), rev(below_sd)),
        col = "blue", border = NA)
points(doy,val_LT, type = "l", lwd = 1, "grey90")
box()
```




<!--chapter:end:chapters/Annual_SST_cycle_indicator.Rmd-->

# Aquaculture

**Description**: Aquaculture indicators

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019)

**Indicator category**: Synthesis of published information

**Contributor(s)**: Sean Hardison, Lisa Calvo, Karl Roscher
  
**Data steward**: Sean Hardison <sean.hardison@noaa.gov>
  
**Point of contact**: Sean Hardison <sean.hardison@noaa.gov> 
  
**Public availability statement**: Source data are publicly available in referenced reports, and are also available for download [here](https://comet.nefsc.noaa.gov/erddap/tabledap/aquaculture_soe_v1.html). 


## Methods
Aquaculture data included in the SOE report were time series of number of oysters sold in Virginia, Maryland, and New Jersey. 


### Data sources
Virginia oyster harvest data are collected from mail and internet-based surveys of active oyster aquaculture operations on both sides of the Chesapeake Bay, which are then synthesized in an annual report [@Hudson2017a]. In Maryland, shellfish aquaculturists are required to report their monthly harvests to the Maryland Department of Natural Resources (MD-DNR). The MD-DNR then aggregates the harvest data for release in the Maryland Aquaculture Coordinating Council Annual Report [@ACC2017], from which data were collected. Similar to Virginia, New Jersey releases annual reports synthesizing electronic survey results from lease-holding shellfish growers. Data from New Jersey reflects cage reared oysters grown from hatchery seed [@Calvo2017]. 


### Data extraction 
Data were collected directly from state aquaculture reports. Oyster harvest data in MD was reported in bushels which were then converted to individual oysters by an estimate of 300 oysters bushel$^{-1}$. View processing code for this indicator [here](https://github.com/NOAA-EDAB/ecodata/blob/master/data-raw/get_aquaculture.R).

### Data analysis
No data analyses occurred for this indicator.

### Plotting

```{r, mariculture, fig.cap="Oyster aquaculture production in terms of number of oysters sold from Virginia, Maryland, and New Jersey.",echo = T, warning = F, message=F, fig.align='center', fig.height=4}

aqua <- ecodata::aquaculture %>% 
  group_by(Var) %>% 
  mutate(hline = mean(Value)) %>%
  ungroup() %>% 
  mutate(Var = plyr::mapvalues(Var, from = c("md oyster harvest","nj oyster harvest","va oyster harvest"),
                                                    to  = c("MD","NJ","VA"))) %>%
  dplyr::rename(State = Var)

aqua$State <- factor(aqua$State, levels = c("VA","MD","NJ"))


ggplot() + 
  geom_segment(aes(x=2005,xend=2017,y=mean(aquaculture[aquaculture$Var == "va oyster harvest",]$Value),
                   yend=mean(aquaculture[aquaculture$Var == "va oyster harvest",]$Value)),
               size = hline.size,
           alpha = hline.alpha,
           linetype = hline.lty,
           color = "#1b9e77",
           inherit.aes = F) +
  geom_segment(aes(x=2012,xend=2016,y=mean(aquaculture[aquaculture$Var == "nj oyster harvest",]$Value),
                   yend=mean(aquaculture[aquaculture$Var == "nj oyster harvest",]$Value)),
               size = hline.size,
           alpha = hline.alpha,
           linetype = hline.lty,
           color = "#d95f02",
           inherit.aes = F) +
  geom_segment(aes(x=2012,xend=2017,y=mean(aquaculture[aquaculture$Var == "md oyster harvest",]$Value),
                   yend=mean(aquaculture[aquaculture$Var == "md oyster harvest",]$Value)),
               size = hline.size,
           alpha = hline.alpha,
           linetype = hline.lty,
           color = "#7570b3",
           inherit.aes = F) +
 #Highlight last ten years
  geom_line(data = aqua, aes(x = Time, y = Value, color = State), size = lwd) +
  geom_point(data = aqua,aes(x = Time, y = Value, color = State), size = pcex) +
  scale_color_manual(values = c(VA = "#1b9e77", MD = "#7570b3",NJ = "#d95f02")) +
  scale_x_continuous(breaks = seq(2005,2018,3),expand = c(0.01, 0.01)) +
  scale_y_continuous(labels = function(l){trans = l / 1000000})+
  ggtitle("Oyster harvest")+
  ylab(expression("Oysters sold (10"^6*" n)")) +
  xlab("")+
  theme_ts()


```





<!--chapter:end:chapters/Aquaculture_indicators.Rmd-->

# Bennet Indicator

**Description**: Bennet Indicator

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) 

**Indicator category**: Database pull with analysis

**Contributor(s)**: John Walden
  
**Data steward**: NA
  
**Point of contact**: John Walden <john.walden@noaa.gov>
  
**Public availability statement**: Derived CFDBS data are available for this analysis (see [Comland](#comdat)).
  
## Methods

### Data sources
Data used in the Bennet Indicator were derived from the Comland data set; a processed subset of the Commercial Fisheries Database Biological Sample (CFDBS). The derived Comland data set is available for download [here](https://comet.nefsc.noaa.gov/erddap/tabledap/group_landings_soe_v1.html).

### Data extraction 
For information regarding processing of CFDBS, please see [Comland](#comdat) methods. The Comland dataset containing seafood landings data was subsetted to US landings after 1964 where revenue was &geq;0 for each EPU (i.e. Mid-Atlantic Bight, Georges Bank, and Gulf of Maine). Each EPU was run in an individual R script, and the code specific to Georges Bank is shown below.

```{r, echo = T, eval = F, warning=F, message=F}
#This code is used to load and process comland data. See comland methods for source data (CFDBS) processing methods. 

#Packages
PKG <- c("data.table","plyr","RColorBrewer", "ggplot2","cowplot","gridExtra","grid")
for (p in PKG) {
  if(!require(p,character.only = TRUE)) {  
    install.packages(p)
    require(p,character.only = TRUE)}
}
# #Setting Save path


#load "comland" data - These data are unavailble due to PII concerns. See aggregated data load below
ecosys2<-subset(comland, US=='TRUE' & YEAR>=1964 & SPPVALUE >=0)

#Load species and PDT codes
load(file.path(data.dir, "Species_codes.RData"))

#Set EPU
epu <- "GB"

#processing
spp<-subset(spp, NESPP3>0)
spp2<-unique(spp[,c(3,12)], by='NESPP3')
spp2<-spp2[which(!duplicated(spp2$NESPP3)),]
sp_combine<-merge(ecosys2, spp2, by="NESPP3", all.x=TRUE)
add.apex <- data.table(NESPP3 = 000, YEAR = 1971, QY = 1, GEAR = 'other',
                       SIZE = 'small', EPU = epu, UTILCD = 0, SPPLIVMT = 0,
                       SPPVALUE = 0, US = TRUE, Feeding.guild = 'Apex Predator')
sp_combine <- rbindlist(list(sp_combine, add.apex))

#Subset data into Georges Bank group
LANDINGS<-subset(sp_combine)
LANDINGS<-LANDINGS[which(!is.na(LANDINGS$Feeding.guild)),]

#Set Up data Table
landsum<-data.table(LANDINGS)
# setkey(landsum,  "EPU", "YEAR","Feeding.guild")
setkey(landsum,"EPU","YEAR","Feeding.guild")


#Sum by feeding guild
landsum[,lapply(.SD, sum, na.rm=TRUE), by=key(landsum), .SDcols=c("SPPLIVMT","SPPVALUE")]

```


### Data analysis

Revenue earned by harvesting resources from an LME at time *t* is a function of both the quantity landed of each species and the prices paid for landings. Changes in revenue between any two years depends on both prices and quantities in each year, and both may be changing simultaneously. For example, an increase in the harvest of higher priced species, such as scallops can lead to an overall increase in total revenue from an LME between time periods even if quantities landed of other species decline. Although measurement of revenue change is useful, the ability to see what drives revenue change, whether it is changing harvest levels, the mix of species landed, or price changes provides additional valuable information. Therefore, it is useful to decompose revenue change into two parts, one which is due to changing quantities (or volumes), and a second which is due to changing prices. In an LME, the quantity component will yield useful information about how the species mix of harvests are changing through time.

A Bennet indicator (BI) is used to examine revenue change between 1964 and 2015 for two major LME regions. It is composed of a volume indicator (VI), which measures changes in quantities, and a price indicator (PI) which measures changes in prices. The Bennet (1920) indicator (BI) was first used to show how a change in social welfare could be decomposed into a sum of a price and quantity change indicator [@Cross2009]. It is called an indicator because it is based on differences in value between time periods, rather than ratios, which are referred to as indices. The BI is the indicator equivalent of the more popular Fisher index [@Balk2010], and has been used to examine revenue changes in Swedish pharmacies, productivity change in U.S. railroads [@lim2009], and dividend changes in banking operations [@Grifell-Tatje2004].  An attractive feature of the BI is that the overall indicator is equal to the sum of its subcomponents [@Balk2010]. This allows one to examine what component of overall revenue is responsible for change between time periods. This allows us to examine whether changing quantities or prices of separate species groups are driving revenue change in each EPU between 1964 and 2015.

Revenue in a given year for any species group is the product of quantity landed times price, and the sum of revenue from all groups is total revenue from the LME. In any year, both prices and quantities can change from prior years, leading to total revenue change. At time t, revenue (R) is defined as $$R^{t} = \sum_{j=1}^{J}p_{j}^{t}y_{j}^{t},$$
where $p_{j}$ is the price for species group $j$, and $y_{j}$ is the quantity landed of species group $j$. Revenue change between any two time periods, say $t+1$ and $t$, is then $R^{t+1}-R^{t}$, which can also be expressed as:
$$\Delta R = \sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t+1}-\sum_{j=1}^{J}p_{j}^{t}y_{j}^{t}.$$
This change can be decomposed further, yielding a VI and PI. The VI is calculated using the following formula [@Moosberg2007]:

$$VI = \frac{1}{2}(\sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t+1} - \sum_{j=1}^{J}p_{j}^{t+1}y_{j}^{t} + \sum_{j=1}^{J}p_{j}^{t}y_{j}^{t+1} - \sum_{j=1}^{J}p_{j}^{t}y_{j}^{t})$$
The price indicator (PI) is calculated as follows:
$$PI = \frac{1}{2}(\sum_{j=1}^{J}y_{j}^{t+1}p_{j}^{t+1} - \sum_{j=1}^{J}y_{j}^{t+1}p_{j}^{t} + \sum_{j=1}^{J}y_{j}^{t}p_{j}^{t+1} - \sum_{j=1}^{J}y_{j}^{t}p_{j}^{t})$$
Total revenue change between time $t$ and $t+1$ is the sum of the VI and PI. Since revenue change is being driven by changes in the individual prices and quantities landed of each species group, changes at the species group level can be examined separately by taking advantage of the additive property of the indicator. For example, if there are five different species groups, the sum of the VI for each group will equal the overall VI, and the sum of the PI for each group will equal the overall PI. 

```{r, echo = T, eval = F}
#R code to construct Bennet Indicator for Ecosystem Project
#Author: John Walden
#Date: October 4, 2017
#
#Revised January 18, 2018 to calculate the indicator relative to average conditions
#during each time period. Set EPU in extraction/processing code chunk above.


#filter by specific EPU
epu = "GB"
value <- subset(landsum, EPU == epu)

#Calculate price
value$PRICE=value$SPPVALUE/value$SPPLIVMT
value[is.na(value)]<-0


#Next two lines are to calculate mean values for landings
#and value for the time series by feeding guild

meanval<-as.data.frame(value[,j=list(mean(SPPVALUE,na.rm=TRUE), mean(SPPLIVMT,na.rm=TRUE)), by=Feeding.guild])
meanval<-rename(meanval, c("V1"="BASEV", "V2"="BASEQ"))
meanval$BASEP=meanval$BASEV/meanval$BASEQ;

#order by feeding guild

value<-value[order(value$Feeding.guild),]
meanval<-meanval[order(meanval$Feeding.guild),]

#Merge Value data frame with Base Year Value Data Frame
value<-merge(value, meanval, by="Feeding.guild")

#Construct price and Volume Indicators
#NOTE: ALL values are normalized to $1,000,000

value$VI=((0.5*(value$BASEP+value$PRICE))*(value$SPPLIVMT-value$BASEQ))/1000000
value$PI=((0.5*(value$BASEQ+value$SPPLIVMT))*(value$PRICE-value$BASEP))/1000000

value<-value[order(value$YEAR),]

#The next Data table sets up the yearly aggregate Bennet PI and VI

biyear<-data.table(value)
setkey(biyear, "YEAR")
biyear<-biyear[,lapply(.SD, sum), by=key(biyear), .SDcols=c("VI","PI","BASEV","SPPVALUE")]
biyear$revchange<-(biyear$VI+biyear$PI)
biyear$BI<-(biyear$VI + biyear$PI)

#The Next Steps restructure the year data frame so the yearly
#Bennet Indicator can be plotted. Negative values are difficult in GGPLOT.
#Since the Bennet indicator can have a negative value, separate data frames
#need to be created. First, the data needs to be restructured to use the 
#stacked bar function in ggplot. GGPLOT is used because it can graph differen#t data layers on the same graph.

y1<-biyear[,c(1,2)]
y1$indicator='VI'
y2<-biyear[,c(1,3)]
y2$indicator='PI'

colnames(y1)[2]<-"value"
colnames(y2)[2]<-"value"
ytotal<-rbind(y1,y2)
```

### Plotting

```{r bennet-plot1, message = F, warning=F}

epu_abbr <- "MAB"

#Filter data into two dataframes for plotting
indicators <- ecodata::bennet %>% 
  filter(EPU == epu_abbr,
         Var %in% c("VI EPU aggregate",
                    "PI EPU aggregate")) %>% 
  mutate(Var, Var = plyr::mapvalues(Var, from = c("VI EPU aggregate","PI EPU aggregate"),
                                    to = c("Volume","Price")))

revchange <- ecodata::bennet %>% 
  filter(EPU == epu_abbr,
         Var %in% c("REVCHANGE EPU aggregate"))

#custom bar fill color (color-blind friendly)
ind_fill <- c("#a6cee3", "#b2df8a")

#limits
y.lim <- c(-450,450)

#plot
ggplot(data = indicators)+
  
  #Highlight last ten years
  annotate("rect", fill = shade.fill, alpha = shade.alpha,
      xmin = x.shade.min , xmax = x.shade.max,
      ymin = -Inf, ymax = Inf)+
  
  geom_bar(aes(x=Time, y= Value, fill = Var), stat="identity")+
  scale_fill_manual(name = "Indicators", values = ind_fill) +
  geom_line(data = revchange, aes(x = Time, y = Value, colour="$"))+
  scale_colour_grey(name ="Revenue Change") +
  ggtitle("Mid-Atlantic Bennet Indicator")+
  labs(y="Value $1,000,000 ($2015)") +
  scale_x_continuous(breaks = seq(1985, 2015, by = 5), expand = c(0.01, 0.01)) +
  scale_y_continuous(breaks = seq(y.lim[1], y.lim[2], by = 100), limits = y.lim, expand = c(0.01, 0.01)) +
  theme_ts() +
  theme(title = element_text(size = 10))
```

<!--chapter:end:chapters/Bennet_indicator.Rmd-->

# Catch and Fleet Diversity

**Description**: Permit-level species diversity and Council-level fleet diversity.

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018), State of the Ecosystem - Mid-Atlantic (2018)

**Indicator category**: Database pull with analysis
Published methods

**Contributor(s)**: Geret DePiper, Min-Yang Lee
  
**Data steward**: Geret DePiper, <geret.depiper@noaa.gov>
  
**Point of contact**: Geret DePiper, <geret.depiper@noaa.gov>
  
**Public availability statement**: Source data is not publicly availabe due to PII restrictions. Derived time series are available for download [here](https://comet.nefsc.noaa.gov/erddap/tabledap/comm_data_soe_v1.html).
  

## Methods
Diversity estimates have been developed to understand whether specialization, or alternatively stovepiping, is occurring in fisheries of the Northeastern Large Marine Ecosystem. We use the average effective Shannon indices for species revenue at the permit level, for all permits landing any amount of [NEFMC](https://www.nefmc.org/) or [MAFMC](http://www.mafmc.org/) Fishery Management Plan (FMP) species within a year (including both Monkfish and Spiny Dogfish). We also use the effective Shannon index of fleet revenue diversity and count of active fleets to assess the extent to which the distribution of fishing changes across fleet segments.

### Data sources
Data for these diversity estimates comes from a variety of sources, including the
Commercial Fishery Dealer Database, Vessel Trip Reports, Clam logbooks, vessel characteristics from Permit database, WPU series producer price index. These data are typically not available to the public.

### Data extraction 
The following describes both the permit-level species and fleet diversity data generation. Price data was extracted from the Commercial Fishery Dealer database (CFDERS) and linked to Vessel Trip Reports by a heirarchical matching algorithm that matched date and port of landing at its highest resolution. Code used in these analyses is available upon request.

<!-- For NOAA personnel: Code currently archived in the \\\\net\\home2\\mlee\\diversity\\code folder, while data is currently archived in \\\\net\\home2\\mlee\\diversity folder. -->

Output data was then matched to vessel characteristics from the VPS VESSEL data set. For the permit-level estimate, species groups are based off of a slightly refined NESPP3 code, defined in the data as "myspp", which is further developed in the script to rectify inconsistencies in the data.

Species groups used include Highly Migratory Species, Monkfish, Atlantic Sea Scallops, Shrimp, Skates, Atlantic Herring, Ocean Quahog, Surf Clam, Tilefish, Black Sea Bass and Fluke, Butterfish and Red Hake and Unknown Whiting, Bluefish, Spiny Dogfish, Illex, American Lobster, Loligo, Menhaden, Offshore hake, Scup, Sand Dabs, Pout, Wolffish, Winter Flounder, Yellowtail Flounder, Unspecified hakes, White hake, Halibut, Bluefish & Scup (NE only), New England Groundfish (cod, pollock, hadddock, Monkfish, Winter flounder, Witch flounder, White hake, Plaice, redfish), Mid-Atlantic Groundfish (cod, wolffish, plaice, Witch flounder, haddock,  pollock, redfish, and halibut), pout and windowpane flounder (MA only), and an "Other" category for all other species. 

For the fleet diversity metric, gears include scallop dredge (gearcodes DRS, DSC, DTC, and DTS), other dredges (gearcodes DRM, DRO, and DRU), gillnet (gearcodes GND, GNT, GNO, GNR, and GNS), hand (gearcode HND), longline (gearcodes LLB and LLP), bottom trawl (gearcodes OTB, OTF, OTO, OTC. OTS, OHS, OTR, OTT, and PTB), midwater trawls (gearcode OTM and PTM), pot (gearcodes PTL, PTW, PTC, PTE, PTF, PTH, PTL, PTO, PTS, and PTX), purse seine (gearcode PUR), and hydraulic clam dredge (gearcode DRC).Vessels were further grouped by length categories of less than 30 feet, 30 to 50 feet, 50 to 75 feet, and 75 feet and above. All revenue was deflated to real dollars using the "WPU0223" Producer Price Index with a base of January 2015. Stata code for data processing is available [here](https://github.com/NOAA-EDAB/tech-doc/tree/master/data/Human_Dimensions_code).

### Data analysis
This permit-level species effective Shannon index is calculated as 
$$exp(-\sum_{i=1}^{N}p_{ijt}ln(p_{ijt}))$$
for all $j$, with $p_{ijt}$ representing the proportion of revenue generated by species or species group $i$ for permit $j$ in year $t$, and is a composite of richness (the number of species landed) and abundance (the revenue generated from each species). The annual arithmetic mean value of the effective Shannon index across permits is used as the indicator of permit-level species diversity. 

In a similar manner, the fleet diversity metric is estimated as 
$$exp(-\sum_{i=1}^{N}p_{kt}ln(p_{kt})) $$
for all $k$, where $p_{kt}$ represents the proportion of total revenue generated by fleet segment $k$ (gear and length combination) per year $t$. The indices each run from 1996 to 2017. A count of the number of fleets active in every year is also provided to assess whether changes in fleet diversity are caused by shifts in abundance (number of fleets), or evenness (concentration of revenue). The work is based off of analysis conducted in @eric_m_thunberg_measures_2015 and published in @gaichas_framework_2016.

```{r fleet-diversity, fig.cap="Fleet diversity (A) and fleet count (B) in the Mid Atlantic Bight.", echo=T, message=FALSE, warning=FALSE, fig.align='center'}

# Relative working directories
data.dir  <- here::here('data')
r.dir <- here::here('R')

# Load data
load(file.path(data.dir,"SOE_data_erddap.Rdata"))

# Source plotting functions
source(file.path(r.dir,"BasePlot_source.R"))

opar <- par(mfrow = c(2, 1), mar = c(0, 0, 0, 0), oma = c(4, 6, 2, 6))

soe.plot(SOE.data, "Time", "Mid-Atlantic average fleet diversity", stacked = "A",
         rel.y.num = 0.9, end.start = 2008, tol = 0.15, full.trend = F, cex.stacked = 1.5)
soe.stacked.axis('Year', 'Fleet diversity', y.line = 2.5, outer = F,
                 rel.x.text = 1, rel.y.text = 1)
soe.plot(SOE.data,"Time", "Mid-Atlantic fleet count", stacked = "B",
         rel.y.num = 0.9, end.start = 2008, full.trend = F, cex.stacked = 1.5)

soe.stacked.axis('Year', 'Fleet count', y.line = 2.5, outer = F,
                 rel.x.text = 1, rel.y.text = 0.95)

```

<!--chapter:end:chapters/Catch_and_Fleet_Diversity_indicators.Rmd-->

# Chlorophyll *a* and Primary Production {#chl-pp}

**Description**: Chlorophyll *a* and Primary Production

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019)

**Indicator category**: Database pull; Database pull with analysis; Published methods

**Contributor(s)**: Kimberly Hyde
  
**Data steward**: Kimberly Hyde, kimberly.hyde@noaa.gov
  
**Point of contact**: Kimberly Hyde, kimberly.hyde@noaa.gov
  
**Public availability statement**: Source data used in these analyses will be made publicly available. Derived data used in State of the Ecosystem Reports can be found [here](http://comet.nefsc.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000).


## Methods
### Data sources
Level 1A ocean color remote sensing data from the Sea-viewing Wide Field-of-view Sensor (SeaWiFS) [@NASA1] on the OrbView-2 satellite and the Moderate Resolution Imaging Spectroradiometer (MODIS) [@NASA2] on the Aqua satellite were acquired from the NASA Ocean Biology Processing Group (OBPG).  Sea Surface Temperature (SST) data included the 4 km nighttime NOAA Advanced Very High Resolution Radiometer (AVHRR) Pathfinder [@Casey2010; @Saha2018] and the Group for High Resolution Sea Surface Temperature (GHRSST) Multiscale Ultrahigh Resolution (MUR, version 4.1) Level 4 [@SOE4; @SOE14] data.  

### Data extraction
NA

### Data analysis
The SeaWiFS and MODIS L1A files were processed using the NASA Ocean Biology Processing Group [SeaDAS](https://seadas.gsfc.nasa.gov/) software version 7.4.  All MODIS files were spatially subset to the U.S. East Coast (SW longitude=-82.5, SW latitude=22.5, NE longitude=-51.5, NE latitude=48.5) using [L1AEXTRACT_MODIS](https://seadas.gsfc.nasa.gov/help/seadas-processing/ProcessL1aextract_modis.html). SeaWiFS files were subset using the same coordinates prior to begin downloaded from the [Ocean Color Web Browser](https://oceancolor.gsfc.nasa.gov/cgi/browse.pl?sen=am).  SeaDAS's [L2GEN](https://seadas.gsfc.nasa.gov/help/seadas-processing/ProcessL2gen.html) program was used to generate Level 2 (L2) files using the default settings and optimal ancillary files, and the [L2BIN](https://seadas.gsfc.nasa.gov/help/seadas-processing/ProcessL2bin.html) program spatially and temporally aggregated the L2 files to create daily Level 3 binned (L3B) files.  The daily files were binned at 2 km resolution that are stored in a global, nearly equal-area, [integerized sinusoidal grids](https://oceancolor.gsfc.nasa.gov/docs/format/l3bins/) and use the default [L2 ocean color flag masks](https://oceancolor.gsfc.nasa.gov/atbd/ocl2flags/).  The global SST data were also subset to the same East Coast region and remapped to the same sinusoidal grid.    

The L2 files contain several ocean color products including the default chlorophyll &alpha; product (CHL-OCI), photosynthetic available radiation (PAR), remote sensing reflectance $(R_{rs}(\lambda))$, and several inherent optical property products (IOPs).  The CHL-OCI product combines two algorithms, the O'Reilly band ratio (OCx) algorithm [@SOE11] and the Hu color index (CI) algorithm [@SOE5].  The SeaDAS default CHL-OCI algorithm diverges slightly from @SOE5 in that the transition between CI and OCx occurs at 0.15 < CI < 0.2 mg m^-3^ to ensure a smooth [transition](https://oceancolor.gsfc.nasa.gov/atbd/chlor_a/). The regional chlorophyll &alpha; algorithm by @SOE12 was used to create a second chlorophyll product (CHL-PAN).  CHL-PAN is an empirical algorithm derived from *in situ* sampling within the Northeast Large Marine Ecosystem (NE-LME) and demonstrated significant improvements from the standard NASA operational algorithm in the NES-LME [@SOE13].  A 3rd-order polynomial function (Equation \@ref(eq:one)) is used to derive [CHL-PAN] from Rrs band ratios (RBR): 

\begin{equation}
log[\textrm{CHL-PAN}] = A_{0} + A_{1}X + A_{2}X^{2} + A_{3}X^{3},  
(\#eq:one) 
\end{equation}

where $X = log(R_{rs}(\lambda_{1})/R_{rs}(\lambda_{2}))$ and $A_{i} (i = 0, 1, 2, \textrm{or }  3)$ are sensor and RBR specific coefficients:

* If SeaWiFS and RBR is $R_{rs}(490)/R_{rs}(555)(R_{^3{\mskip -5mu/\mskip -3mu}_5})$ then: $A_0=0.02534, A_1=-3.033, A_2=2.096, A_3=-1.607$
* If SeaWiFS and RBR is $R_{rs}(490)/R_{rs}(670)(R_{^3{\mskip -5mu/\mskip -3mu}_6})$  then: $A_0=1.351, A_1=-2.427, A_2=0.9395, A_3=-0.2432$
* If MODIS and RBR is $R_{rs}(488)/R_{rs}(547)(R_{^3{\mskip -5mu/\mskip -3mu}_5})$  then: $A_0=0. 03664, A_1=-3.451, A_2=2.276, A_3=-1.096$
* If MODIS and RBR is $R_{rs}(488)/R_{rs}(667)(R_{^3{\mskip -5mu/\mskip -3mu}_6})$  then: $A_0=1.351, A_1=-2.427, A_2=0.9395, A_3=-0.2432$

C~3/5~ and C~3/6~ were calculated for each sensor specific RBR (R~3/5~ and R~3/6~ respectively) and then the following criteria were used to determine to derive CHL-PAN:

<ol type="a">
  <li>If $R_{^3{\mskip -5mu/\mskip -3mu}_5}>0.15$ or $R_{6} <0.0001$ then $\textrm{CHL-PAN} = C_{^3{\mskip -5mu/\mskip -3mu}_5};$</li>
  <li> Otherwise, $\textrm{CHL-PAN} = \textrm{max}(C_{^3{\mskip -5mu/\mskip -3mu}_5}, C_{^3{\mskip -5mu/\mskip -3mu}_6})$,</li>
</ol>

where $R_6$ is $R_{rs}(670)$ (SeaWiFS) or $R_{rs}(667)$ [@SOE13]. 

The Vertically Generalized Production Model (VGPM) estimates net primary production (PP) as a function of chlorophyll &alpha;, photosynthetically available light and the photosynthetic efficiency [@SOE1].  In the VGPM-Eppley version, the original temperature-dependent function to estimate the chlorophyll-specific photosynthetic efficiency is replaced with the exponential "Eppley" function (equation PP1) as modified by @SOE7. The VGPM calculates the daily amount of carbon fixed based on the maximum rate of chlorophyll-specific carbon fixation in the water column, sea surface daily photosynthetically available radiation, the euphotic depth (the depth where light is 1% of that at the surface), chlorophyll &alpha; concentration, and the number of daylight hours (Equation \@ref(eq:two)).  

\begin{equation}
P_{max}^{b}(SST) = 4.6 * 1.065^{SST-20^{0}} 
(\#eq:two) 
\end{equation}
Where $P_{max}^{b}$ is the maximum carbon fixation rate and *SST* is sea surface temperature.

\begin{equation}
PP_{eu} = 0.66125 * P_{max}^{b} * \frac{I_{0}}{I_{0}+4.1} * Z_{eu} * \textrm{CHL} * \text{DL}
(\#eq:three) 
\end{equation}

Where $PP_{eu}$ is the daily amount of carbon fixed integrated from the surface to the euphotic depth (mgC m^-2^ day^-1^), $P_{max}^{b}$ is the maximum carbon fixation rate within the water column (mgC mgChl^-1^ hr^-1^), $I_{0}$ is the daily integrated molar photon flux of sea surface PAR (mol quanta m^-2^ day^-1^), Zeu is the euphotic depth (m), CHL is the daily interpolated CHIi-OCI (mg m^-3^), and DL is the photoperiod (hours) calculated for the day of the year and latitude according to @SOE6. The light dependent function $(I_{0}/(I_{0}+4.1))$ describes the relative change in the light saturation fraction of the euphotic zone as a function of surface PAR ($I_0$).  Zeu is derived from an estimate of the total chlorophyll concentration within the euphotic layer (*CHL~eu~*) based on the Case I models of @SOE8:

* For $\textrm{CHL}_{eu} > 10.0\;\;\;\;\;Z_{eu} = 568.2 * \textrm{CHL}_{eu}^{-0.746}$
* For $\textrm{CHL}_{eu} \leq 10.0\;\;\;\;\;Z_{eu} = 200.0 * \textrm{CHL}_{eu}^{-0.293}$
* For $\textrm{CHL}_{0} \leq 1.0\;\;\;\;\;\textrm{CHL}_{eu} = 38.0 * \textrm{CHL}_{0}^{0.425}$
* For $\textrm{CHL}_{0} > 1.0\;\;\;\;\;\textrm{CHL}_{eu} = 40.2 * \textrm{CHL}_{0}^{0.507}$

Where $\textrm{CHL}_0$ is the surface chlorophyll concentration.

Prior to being input into the VGPM-Eppley model, the daily CHL-OCI and AVHRR SST data were temporally interpolated and smoothed (CHL-OCI~INT~ and SST~INT~ respectively) to increase the data coverage and better match data collected from different sensors and different times.  The daily PAR data are not affected by cloud cover and MUR SST data is a blended/gap free data product so these products were not interpolated.  

Daily data at each pixel location covering the entire date range were extracted to create a pixel time series $(D_{x,y})$.  $(D_{x,y})$ are linearly interpolated based on days in the time series using [interpx.pro](https://github.com/callumenator/idl/blob/master/external/JHUAPL/INTERPX.PRO). Prior to interpolation, the CHL data are log-transformed to account for the log-normal distribution of chlorophyll data [@SOE2].  Interpolating the entire times series requires a large amount of processing time so the series was processed one year at a time.  Each yearly series included 60 days from the previous year and 60 days from the following year to improve the interpolation at the beginning and end of the year.  Following interpolation, the data are smoothed with a tri-cube filter (width=7) using IDL's [CONVOL](https://www.harrisgeospatial.com/docs/CONVOL.html) program.  In order to avoid over interpolating data when there were several days of missing data in the time series, the interpolated data were removed and replaced with blank data if the window of interpolation spanned more than 7 days for CHL or 10 days for SST.  After all D~x,y~ pixels had been processed, the one-dimensional pixel time series were converted back to two-dimensional daily files. 

Statistics, including the arithmetic mean, geometric mean (for CHL and PP), standard deviation, and coefficient of variation were calculated at daily (3 and 8-day running means), weekly, monthly, and annual time steps and for several climatological periods.  Annual statistics used the monthly means as inputs to avoid a summer time bias when more data is available due to reduced cloud cover.  The daily, weekly, monthly and annual climatological statistics include the entire time series for each specified period.  For example, the climatological January uses the monthly mean from each January in the time series and the climatological annual uses the annual mean from each year.  The CHL and PP climatological statistics include data from both SeaWiFS (1997-2007) and MODIS (2008-2017).  

Weekly, monthly and annual anomalies were calculated for each product by taking the difference between the mean of the input time period (i.e. week, month, year) and the climatological mean for the same period.  Because bio-optical data are typically log-normally distributed [@SOE2], the CHL and PP data were first log-transformed prior to taking the difference and then untransformed, resulting in an anomaly ratio.

The ecological production unit (EPU) shapefile that excludes the estuaries was used to spatially extract all data location within an ecoregion from the statistic and anomaly files.  The median values, which are equivalent to the geometric mean, were used for the CHL and PP data.  For the extended time series, the 1998-2007 data use the SeaWiFS ocean color products and MODIS-Aqua products were used from 2008 to 2017.  Prior to June 2002, AVHRR Pathfinder data are used as the SST source and MUR SST in subsequent years.

### Plotting

The following figures show examples of how Chlorophyll *a* and primary production data have been included into State of the Ecosystem reports. The figure immediately below shows primary production anomaly plotted with the small-large copepod index (see [Zooplankton](#zooabund)) in the Mid-Atlantic Bight. 

```{r, MAB-zooplankton, fig.cap="MAB small-large zooplankton index and the annual primary production anomaly.", message=F, warning=F}

epu_abbr <- "MAB"

sli <- ecodata::zoo_anom_sli %>% 
  filter(EPU == epu_abbr,
         str_detect(Var, "small-large")) %>% 
  mutate(hline = 0) 

pp_anom <- ecodata::chl_pp %>% 
  filter(str_detect(Var, "ANNUAL_PPD_RATIO_ANOMALY"),
         EPU == "MAB") %>% 
  mutate(hline = 1,
         Time = as.numeric(as.character(Time)),
         Var = "ANNUAL_PPD_RATIO_ANOMALY")

sli_plt <- sli %>% 
  ggplot(aes(x = Time, y = Value, group = Var)) +
         annotate("rect", fill = shade.fill, alpha = shade.alpha,
      xmin = x.shade.min , xmax = x.shade.max,
      ymin = -Inf, ymax = Inf) +
  geom_line() +
  geom_point() +
  guides(color = F) +
  xlab("")+
  ylab("Small-large abundance") +
  ggtitle("Small-large copepod abundance") +
    scale_x_continuous(expand = c(0.01, 0.01), limits = c(1988, 2018))+
      geom_hline(aes(yintercept = hline,
                     group = Var),
           size = hline.size,
           alpha = hline.alpha,
           linetype = hline.lty)+
  theme_ts() +
  theme(strip.text=element_text(hjust=0,
                                face = "italic"))

pp_anom_plt <- pp_anom %>% 
    ggplot(aes(x = Time, y = Value, group = Var)) +
         annotate("rect", fill = shade.fill, alpha = shade.alpha,
      xmin = x.shade.min , xmax = x.shade.max,
      ymin = -Inf, ymax = Inf) +
  geom_line() +
  geom_point() +
  guides(color = F) +
  ylab("Anomaly ratio") +
  ggtitle("Primary production anomaly ratio") +
    scale_x_continuous(expand = c(0.01, 0.01), limits = c(1988, 2018))+
    scale_y_continuous(limits = c(0.8,1.2)) +
      geom_hline(aes(yintercept = hline,
                     group = Var),
           size = hline.size,
           alpha = hline.alpha,
           linetype = hline.lty)+
  theme_ts() +
  theme(strip.text=element_text(hjust=0,
                                face = "italic"))


sli_plt + pp_anom_plt + plot_layout(ncol = 1) & theme(plot.margin = margin(t = 0))

```

Chl *a* and primary production data were also examined in relation to the long-term means of each series. The figure below shows data specific to the Mid-Atlantic Bight. 

```{r SST-PPD-CHL-MAB, fig.cap="Weekly chlorophyll concentrations in the Mid-Atlantic are shown by the colored line for 2018. The long-term mean is shown in black, and shading indicates +/- 1 sample SD.",echo = T, fig.show='hold', fig.align='default',warning = F, message = F,fig.pos='H',fig.height = 4,fig.width = 8}

interp_chl_pp <- function(epu, year = 2018, Variable){
  out <- ecodata::chl_pp %>% 
    filter(str_detect(Var,Variable),
           EPU == epu) %>% 
    separate(.,Time, c("Year","Week"),sep = 4) %>% 
    filter(Year == year) %>% 
    group_by(EPU) %>% 
    mutate(Time = 1:length(Year))
  
  ltm_out <- ecodata::chl_pp %>% 
    filter(str_detect(Var,Variable),
           EPU == epu) %>% 
    separate(.,Time, c("Year","Week"),sep = 4) %>% 
    group_by(Week) %>% 
    dplyr::summarise(LTM = mean(Value, na.rm = T),
                     SD = sd(Value, na.rm = T)) %>% 
    mutate(Time = 1:length(Week),
           sd.low = LTM - SD,
           sd.high = LTM + SD) %>% 
    left_join(.,out, by = c("Time")) %>% 
    mutate(status = ifelse(Value < sd.high & Value > sd.low, "near_mean",
                           ifelse(Value > sd.high, "high",
                                  ifelse(Value < sd.low,"low",NA))),
           group = "PLOT")
  
  return(ltm_out)
}


MAB_chl <- interp_chl_pp(epu = "MAB", Variable = "WEEKLY_CHLOR_A_MEDIAN MODIS-Aqua PAN")

MAB_chl_weekly <- ggplot(data = MAB_chl) +
  geom_line(aes(x = Time, y = LTM)) +
  geom_ribbon(aes(x = Time, ymin = sd.low, ymax = sd.high), 
              alpha = 0.1,
              fill = "grey1") +
  geom_line(aes(x = Time, y = Value),
            size = 1,color = "#33a02c") +
  ggtitle(expression("Chlorophyll"~italic(a)~"")) +
  guides(color = F) +
  xlab("")+
  ylab(expression("CHL (mg m"^-3*")")) +
  scale_x_continuous(breaks = seq(1,52,10),
                   labels = c("Jan.","Mar.","May","July","Oct.","Dec."),
                   expand = c(0.01,0.01)) +
  scale_color_manual(values = c("#ef8a62","#2c7fb8","#a1d99b"))+
  theme_ts()

MAB_pp <- interp_chl_pp(epu = "MAB", Variable =  "WEEKLY_PPD_MEDIAN")

MAB_pp_weekly <- ggplot(data = MAB_pp) +
  geom_line(aes(x = Time, y = LTM)) +
  geom_ribbon(aes(x = Time, ymin = sd.low, ymax = sd.high),
              alpha = 0.1,
              fill = "grey1") +
  geom_line(aes(x = Time, y = Value),
            size = 1,color = "#33a02c") +
  ggtitle(expression("Primary production")) +
  guides(color = F) +
  xlab("")+
  ylab(expression("PP (gC m"^-2*" d"^-1*")")) +
  scale_x_continuous(breaks = seq(1,52,10),
                   labels = c("Jan.","Mar.","May","July","Oct.","Dec."),
                   expand = c(0.01,0.01)) +
  scale_color_manual(values = c("#ef8a62","#2c7fb8","#a1d99b"))+
  theme_ts()

#MAB_chl_weekly + MAB_pp_weekly + plot_layout(ncol = 1)
MAB_chl_weekly
```

In the figure below, we show monthly primary productivity on an annual time step in the Mid-Atlantic Bight. 

```{r PP-OCCI, fig.width=8, fig.cap = "Monthly primary production trends show the annual cycle (i.e. the peak during the summer months) and the changes over time for each month."}
out_pp <- ecodata::chl_pp %>% 
  filter(EPU == epu_abbr,
         str_detect(Var, "MONTHLY_PPD_MEDIAN")) %>% 
  separate(.,Time, into = c("Year","Month"), sep = 4) %>% 
    mutate(Month = plyr::mapvalues(Month, from = c("01","02","03","04","05","06",
                                                   "07","08","09","10","11","12"),
                                   to = c(month.abb))) %>% 
  group_by(Month) %>% 
  mutate(hline = mean(Value))
out_pp$Month <- factor(out_pp$Month, levels = month.abb)


(pp_cci <- ggplot(out_pp) +  
   # geom_gls(aes(x = Year, y = Value, group = Month))+
    geom_point(aes(x = Year, y = Value, group = Month)) +
    geom_line(aes(x = Year, y = Value, group = Month)) +
    scale_x_discrete(name = "Time", breaks = seq(min(out_pp$Year),max(out_pp$Year),10)) +  
        geom_hline(aes(yintercept = hline),
           size = hline.size,
           alpha = hline.alpha,
           linetype = hline.lty) +
    facet_wrap(Month~., ncol = 6) +
    ggtitle("Monthly median PPD") +
    ylab(expression("PP (gC m"^-2*" d"^-1*")")) +
    theme_facet() +
    theme(axis.text.x = element_text(angle=45, hjust = 1),
          panel.spacing = unit(1, "lines"),
          plot.margin = unit(c(0.1, 0, 0, 0), "cm")))
```

<!--chapter:end:chapters/CHL_PPD_indicator.Rmd-->

# Fishing Community Climate Vulnerability

**Description**: Community climate vulnerability

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018), State of the Ecosystem - Mid-Atlantic (2018) 

**Indicator category**: Database pull with analysis

**Contributor(s)**: Lisa L. Colburn
  
**Data steward**: Lisa L. Colburn
  
**Point of contact**: Lisa L. Colburn
  
**Public availability statement**: The fisheries data used for this analysis includes confidential information and is not available to the public. 

  

## Methods
### Data sources
The data used in community climate vulnerability analyses were derived from the following sources in partnership with the Atlantic Coastal Cooperative Statistics Program's (ACCSP) Standard Atlantic Fisheries Information System (SAFIS).

```{r source table, echo = F, include = T, results='asis'}
tabl <- '
|Database Name     | Description                                  |
|:-------------------------|:-------------------------------------|
|Cfdersyyyy|The dealer data are transaction-level pricing at the level of the "market-category." These data are primarily generated through mandatory reporting by federally-permitted fish dealers. The federal reporting is supplemented with data from non-federally-permitted (state-only) fish dealers. Data are currently reported electronically in partnership with ACCSP through SAFIS.           |
|Cfvessyyy|A related database that contains permit information.   |

'
cat(tabl)
```

In these databases, the variable "port" contains the post associated with the vessel. The variable "Statenm" refers to the state of the mailing address of the owner.

### Data extraction 
```{sql, eval = F, echo = T}
create table cfders2011 as
select *
from connection to oracle
(select port, state, year, dealnum, permit, nespp3, spplndlb, sppvalue from cfders2011 where permit > 0 order by permit);
create table cfvess11 as
select *
from connection to oracle
(select permit, homeport, homest from CFDBS.cfvess11 where permit > 0 order by permit);
create table port_name as
select *
from connection to oracle
(select port, portnm from port order by port);
create table st_name as
select *
from connection to oracle
(select state, stateabb from statenm order by state);

Truncated SAS code:
/*CREATE VARIABLES FOR TOTAL LANDINGS WEIGTH AND VALUE (SUM) BY PORT OF LANDING AND BY HOMEPORT*/
data landings_ports1; set landings_ports;
run;
proc sort;
by port state;
run;
proc means noprint data = landings_ports1; by port state; 
var spplndlb sppvalue;
id port state;
output out = landport_totspp sum = L_Totlb L_Totval;
run;
proc sort;
by port;
run;
data landings_ports2; set landings_ports;
run;
proc sort;
by homeport homest;
run;
proc means noprint data = landings_ports2; by homeport homest; 
var spplndlb sppvalue;
id homeport homest;
output out = homeport_totspp sum = H_Totlb H_Totval;
run;
proc sort;
by homeport;
run;
/*CREATE SPECIES VARIABLES*/
data landings_ports_NE_spp; set landings_ports;
monklb = 0; monkval = 0; /*monkfish*/
bluelb = 0; blueval = 0; /*bluefish*/
.omitted.
otherlb = 0; otherval = 0; /*other - everything else*/
run;
data landings_ports_NE_spp2; set landings_ports_NE_spp;
if nespp3 = 012 then do; monklb = spplndlb; monkval = sppvalue; end;
...ommitted.
if nespp3 = 406 then do; spotlb = spplndlb; spotval = sppvalue; end;
if nespp3 not in (012, 023, 033, 051, 081, 105, 112, 115, 116, 120, 121, 122, 123, 124, 125, 132, 147, 152, 153, 155, 159, 168, 194, 197, 212, 
221, 240, 250, 269, 305, 329, 330, 335, 344, 345, 351, 352, 365, 366, 367, 368, 369, 370, 372, 373, 384, 415, 418, 432, 438, 443, 444, 445, 
446, 447, 464, 466, 467, 468, 469, 470, 471, 472, 507, 508, 509, 512, 517, 700, 710, 711, 724, 727, 748, 754, 769, 774, 775, 781, 786, 789, 
798, 799, 800, 801, 802, 805, 806, 899, 001, 090, 069, 107, 150, 173, 196, 334, 347, 349, 364, 371, 420, 422, 481, 484, 714, 776, 777, 823, 763, 736) 
then do; otherlb = spplndlb; otherval = sppvalue; end;
run; 
/*SUM SPECIES LANDINGS BY PORT OF LANDING*/
proc sort; by port; proc means noprint data = landings_ports_NE_spp2; by port state; 
. omitted ...
id port state;
output out = spp_porlnd_NE sum = ;
run;
proc sort;
by port;
run;
/*SUM SPECIES LANDINGS BY HOMEPORT*/
data spp_home; set landings_ports_NE_spp2;
run;
proc sort; by homeport homest; proc means noprint data = spp_home; by homeport homest; 
. species are counted..
id homeport homest;
output out = spp_homep_NE sum = ;
run
proc sort;
by homeport; run;
/*MERGE TOTAL PERMITS AND TOTAL DEALERS BY PORT OF LANDING*/
data land_port_totperm2; set land_port_totperm;
run;
proc sort;
by port; run;
data lnd_port_permit; merge spp_porlnd_NE (IN=X) land_port_totperm2 (IN=Y);
by port; if X=1; run;data land_port_totdeal2; set land_port_totdeal;
run;
proc sort;
by port;
run;
data lnd_port_permit_deal; merge lnd_port_permit (IN=x) land_port_totdeal2 (IN=Y);
by port; if X=1; run;
/*MERGE WITH PORT NAME AND STATE ABBREVIATION*/
data lnd_port_permit_deal_nm; merge lnd_port_permit_deal (IN=X) port_name (IN=Y);
by port; if X=1; run;
data lnd_port_permit_deal_nm_st; merge lnd_port_permit_deal_nm (IN=x) st_name (IN=Y); proc sort;
by port; if X=1; run;
/*MERGE TOTAL PERMITS AND TOTAL DEALERS BY HOMEPORT*/
data home_port_totperm2; set home_port_totperm;
run;
proc sort;
by homeport;
run;
data home_port_permit; merge spp_homep_NE (IN=X) home_port_totperm2 (IN=Y);
by homeport; if X=1; run; data home_port_totdeal2; set home_port_totdeal;
run;
proc sort;
by homeport;
run;
data home_port_permit_deal; merge home_port_permit (IN=x) home_port_totdeal2 (IN=Y);
by homeport; if X=1; run; proc sort;
by homeport;
run;
/*MERGE TOTAL LANDINGS BY PORT OF LANDING*/
data lnd_port_per_deal_nm_st_tspp; merge lnd_port_permit_deal_nm_st (IN=X) landport_totspp (IN=Y);
by port; if X=1; run;
/*MERGE TOTAL LANDINGS BY HOMEPORT*/
data home_port_per_deal_tspp; merge home_port_permit_deal (IN=X) homeport_totspp (IN=Y);
by homeport; if X=1; run;
data netana.port_landing11; set lnd_port_per_deal_nm_st_tspp;
if state in (22, 32, 24, 42, 7, 35, 33, 8, 23, 49, 36);
run;
proc sort;
by port state;
run;
data netana.homeport11; set home_port_per_deal_tspp;
if homest in ('ME', 'NH', 'MA', 'RI', 'CT', 'NY', 'NJ', 'DE', 'MD', 'VA', 'NC');
run;
proc sort;
by homeport homest;
run;


```

### Data analysis and plotting

The results described below were developed using the methodology described in @colburn_indicators_2016.

1.  *Mapping community climate vulnerability* - The map was produced using two variables: total value landed in a community and community species vulnerability, defined below:
    a.  Communities were grouped based on total value of landings into the following categories: 1 (<\$ 200,000), 2 (\$200,000-\$9,999,999), 3 (\$10,000,000-\$49, 999,999), and 4 (\$50,000,000 and above). Only communities with a total value landed of \$200,000 or more were selected for the mapping process.
    b.  Community climate vulnerability is determined by the percent contribution of species classified as very high, high, moderate, or low climate vulnerability in a community. The percent contribution of species is calculated as following: 


        * **% VH & H** = value of landing contributed by species classified as having very high or high climate change vulnerability/total value of landings \* 100

        * **% M** = value of landing contributed by species classified as having moderate climate 
change vulnerability/total value of landings \* 100

        * **% L** = value of landing contributed by species classified as having low climate change 
vulnerability/ total value of landings \* 100

If a community received a dominant score (50% or more) for any of the above categories, % VH &, %M, or %L, then the community received a respective community species vulnerability ranking of High, Moderate, or Low.  For example, if 90% of the total value landed a community is contributed by species classified as having very high or high climate change vulnerability, then this community gets "Very High/High" community species vulnerability. In case of no dominant percentage identified, the community gets a "Mixed" community species vulnerability ranking. 

2)  *Pie charts* - The pie charts were created using the NMFS landings data pulled from NEFSC databases in Woods Hole, MA. The percent contribution of each species was calculated by dividing the total value of landings in each port by each species' landed value.  Data was calculated and graphed in a pie chart in Excel and given the colors that represent High (red), Moderate (blue), Low (yellow) climate vulnerability. The "other" category consists of species with low landings and/or those that do not have a vulnerability ranking based on @Hare2016.  These species were aggregated and given the color gray.


```{r species-vulnerability, fig.cap="Commercial species vulnerability to climate change in in New England fishing communities.",fig.align = 'center', echo = F, eval = T}
knitr::include_graphics(file.path(image.dir, 'Species_vulnerability_NE.jpg'))
```




<!--chapter:end:chapters/Comm_climate_vuln_indicator.Rmd-->

# Conceptual Models

**Description**: Conceptual models for the New England (Georges Bank and Gulf of Maine) and Mid-Atlantic regions of the Northeast US Large Marine Ecosystem

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018,2019), State of the Ecosystem - Mid-Atlantic (2018,2019) 

**Indicator category**: Synthesis of published information, Extensive analysis; not yet published

**Contributor(s)**: Sarah Gaichas, Patricia Clay, Geret DePiper, Gavin Fay, Michael Fogarty, Paula Fratantoni, Robert Gamble, Sean Lucey, Charles Perretti, Patricia Pinto da Silva, Vincent Saba, Laurel Smith, Jamie Tam, Steve Traynor, Robert Wildermuth 

**Data steward**: Sarah Gaichas, <sarah.gaichas@noaa.gov>

**Point of contact**: Sarah Gaichas, <sarah.gaichas@noaa.gov>

**Public availability statement**: All source data aside from confidential commercial fisheries data (relevant only to some components of the conceptual models) are available to the public (see Data Sources below).


## Methods
Conceptual models were constructed to facilitate multidisciplinary analysis and discussion of the linked social-ecological system for integrated ecosystem assessment. The overall process was to first identify the components of the model (focal groups, human activities, environmental drivers, and objectives), and then to document criteria for including groups and linkages and what the specific links were between the components.

The prototype conceptual model used to design Northeast US conceptual models for each ecosystem production unit (EPU) was designed by the California Current IEA program. The California Current IEA developed an [overview conceptual model for the Northern California Current Large Marine Ecosystem (NCC)](https://www.integratedecosystemassessment.noaa.gov/Assets/iea/california/conceptual-models/Integrated-SocioEcological-System-Overview6.png), with models for each [focal ecosystem component](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/focal-components/coastal-pelagic-overview.html#) that detailed the [ecological](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/focal-components/coastal-pelagic-ecological.html#), [environmental](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/focal-components/coastal-pelagic-environmental.html#), and [human system](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/focal-components/coastal-pelagic-human.html#) linkages. Another set of conceptual models outlined [habitat](https://www.integratedecosystemassessment.noaa.gov/regions/california-current-region/components/mediating-components/habitat.html) linkages. 

An inital conceptual model for Georges Bank and the Gulf of Maine was outlined at the 2015 ICES WGNARS meeting. It specified four categories: Large scale drivers, focal ecosystem components, human activities, and human well being. Strategic management objectives were included in the conceptual model, which had not been done in the NCC. Focal ecosystem components were defined as aggregate species groups that had associated US management objectives (outlined within WGNARS for IEAs, see @depiper_operationalizing_2017): groundfish, forage fish, fished invertebrates, living habitat, and protected species. These categories roughly align with Fishery Managment Plans (FMPs) for the New England Fishery Management Council. The Mid-Atlantic conceptual model was developed along similar lines, but the Focal groups included demersals, forage fish, squids, medium pelagics, clams/quahogs, and protected species to better align with the Mid Atlantic Council's FMPs.

```{r draftmod, echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'GBGOMconceptual1.png'))
```

After the initial draft model was outlined, working groups were formed to develop three submodels following the CCE example: ecological, environmental, and human dimensions. The general approach was to specify what was being included in each group, what relationship was represented by a link between groups, what threshold of the relationship was used to determine whether a relationship was significant enough to be included (we did not want to model everything), the direction and uncertainty of the link, and documentation supporting the link between groups. This information was recorded in a [spreadsheet](https://comet.nefsc.noaa.gov/erddap/tabledap/concept_model_2018.html). Submodels were then merged together by common components using the "merge" function in the (currently unavailable) desktop version of Mental Modeler (http://www.mentalmodeler.org/#home; @gray_mental_2013). The process was applied to Georges Bank (GB), the Gulf of Maine (GOM), and the Mid-Atlantic Bight (MAB) [Ecological Production Units](#epu). 

### Data sources

#### Ecological submodels
Published food web (EMAX) models for each subregion [@link_documentation_2006; @link_northeast_2008], food habits data collected by NEFSC trawl surveys [@smith_trophic_2010], and other literature sources [@smith_consumption_2015] were consulted. Expert judgement was also used to adjust historical information to current conditions, and to include broad habitat linkages to Focal groups. 

#### Environmental submodels
Published literature on the primary environmental drivers (seasonal and interannual) in each EPU was consulted. 
Sources for Georges Bank included @backus_georges_1987 and @townsend_oceanography_2006. 
Sources for the Gulf of Maine included @smith_mean_1983, @smith_interannual_2001, @mupparapu_role_2002, @townsend_oceanography_2006, @smith_regime_2012, and @mountain_labrador_2012.  
Sources for the Mid Atlantic Bight included @houghton_middle_1982, @beardsley_nantucket_1985, @lentz_climatology_2003, @mountain_variability_2003,   @glenn_biogeochemical_2004, @sullivan_evidence_2005, @castelao_seasonal_2008, @shearman_long-term_2009, @castelao_temperature_2010, @gong_seasonal_2010, @gawarkiewicz_direct_2012, @forsyth_recent_2015, @fratantoni_description_2015, @zhang_dynamics_2015, @miller_state-space_2016, and @lentz_seasonal_2017.

#### Human dimensions submodels
Fishery catch and bycatch information was drawn from multiple regional datasets, incuding the Greater Atlantic Regional Office Vessel Trip Reports & Commercial Fisheries Dealer databases, Northeast Fishery Observer Program & Northeast At-Sea Monitoring databases, Northeast Fishery Science Center Social Sciences Branch cost survey, and the Marine Recreational Informational Program database. Further synthesis of human welfare derived from fisheries was drawn from @fare_adjusting_2006, @walden_productivity_2012, @lee_inverse_2013, @lee_hedonic_2014, and @lee_applying_2017. Bycatch of protected species was taken from @waring_us_2015, with additional insights from @bisack_measuring_2014. The top 3 linkages were drawn for each node. For example, the top 3 recreational species for the Mid-Atlantic were used to draw linkages between the recreational fishery and species focal groups. A similar approach was used for relevant commercial fisheries in each region.

Habitat-fishery linkages were drawn from unpublished reports, including:  

1. Mid-Atlantic Fishery Management Council. 2016. Amendment 16 to the Atlantic Mackerel, Squid, and Butterfish Fishery Management Plan: Measures to protect deep sea corals from Impacts of Fishing Gear. Environmental Assessment, Regulatory Impact Review, and Initial Regulatory Flexibility Analysis. Dover, DE. August, 2016. 

2. NOAA. 2016. Deep sea coral research and technology program 2016 Report to Congress. http://www.habitat.noaa.gov/protection/corals/deepseacorals.html retrieved February 8, 2017.  

3. New England Fishery Management Council. 2016. Habitat Omnibus Deep-Sea Coral Amendment: Draft. http://www.nefmc.org/library/omnibus-deep-sea-coral-amendment Retrieved Feb 8, 2017.

4. Bachman et al. 2011. The Swept Area Seabed Impact (SASI) Model: A Tool for Analyzing the Effects of Fishing on Essential Fish Habitat. New England Fisheries Management Council Report. Newburyport, MA.

Tourism and habitat linkages were drawn from unpublished reports, including: 

1. http://neers.org/RESOURCES/Bibliographies.html                               

2. Great Bay (GoM) resources  http://greatbay.org/about/publications.htm        

3. Meaney, C.R. and C. Demarest. 2006. Coastal Polution and New England Fisheries. Report for the New England Fisheries Management Council. Newburyport, MA.

4. List of valuation studies, by subregion and/or state, can be found at http://www.oceaneconomics.org/nonmarket/valestim.asp.

Published literature on human activities in each EPU was consulted. 

Sources for protected species and tourism links included @hoagland_demand_2000 and @lee_economic_2010. 

Sources for links between environmental drivers and human activities included @adams_uncertainty_1973, @matzarakis_proceedings_2001, @scott_climate_2004, @hess_climate_2008, @colburn_social_2012, @jepson_development_2013, and @colburn_indicators_2016. 

Sources for cultural practices and attachments links included @pauly_putting_1997, @mcgoodwin_understanding_2001, @st_martin_making_2001, @norris-raynbird_for_2004, @pollnac_toward_2006, @clay_defining_2007, @clay_definingfishing_2008, @everett_role_2008, @donkersloot_politics_2010, @lord_understanding_2011, @halpern_index_2012, @wynveen_natural_2012, @cortes-vazquez_identity_2013, @koehn_progress_2013, @potschin_landscapes_2013, @reed_beyond_2013, @urquhart_constructing_2013, @blasiak_paradigms_2014, @klain_what_2014, @poe_cultural_2014, @brown_we_2015, @donatuto_evaluating_2015, @khakzad_role_2016, @oberg_surviving_2016, and @seara_perceived_2016.  

### Data extraction 

#### Ecological submodels
"Data" included model estimated quantities to determine whether inclusion thresholds were met for each potential link in the conceptual model. A matrix with diet composition for each modeled group is an input to the food web model. A matrix of mortalities caused by each predator and fishery on each modeled group is a direct ouput of a food web model (e.g. Ecopath). Food web model biomasss flows between species, fisheries, and detritus were summarized using algorithms implemented in visual basic by Kerim Aydin, NOAA NMFS Alaska Fisheries Science Center. Because EMAX model groups were aggregated across species, selected diet compositions for individual species were taken from the NEFSC food habits database using the FEAST program for selected species (example query below). These diet queries were consulted as supplemental information. 

Example FEAST sql script for Cod weighted diet on Georges Bank. Queries for different species are standardized by the FEAST application and would differ only in the svspp code. 
```{sql FEAST, eval = F, echo = T}
Select svspp,year,cruise6,stratum,station,catsex,pdid,pdgutw,pdlen,pdwgt,perpyw,pyamtw,COLLCAT,numlen,pyamtv  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by svspp,year,cruise6,stratum,station,pdid,COLLCAT
Select distinct svspp,year,cruise6,stratum,station from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by svspp,year,cruise6,stratum,station
Select distinct svspp,year,cruise6,stratum,station,catsex,catnum from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by svspp,year,cruise6,stratum,station
Select distinct COLLCAT from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by COLLCAT
Select distinct svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,pdgutw,pdwgt  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by svspp,year,cruise6,stratum,station,catsex,pdid
Select svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT,sum(perpyw),sum(pyamtw),sum(pyamtv)  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT order by svspp,year,cruise6,stratum,station,catsex,pdid,pdlen,COLLCAT
Select svspp,year,cruise6,stratum,station,COLLCAT,sum(pyamtv) sumpvol from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,COLLCAT order by svspp,year,cruise6,stratum,station,COLLCAT
Select svspp,year,cruise6,stratum,station, count(distinct pdid) nstom  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,catsex order by svspp,year,cruise6,stratum,station
Select svspp,year,cruise6,stratum,station,pdlen,numlen,count(distinct pdid) nstom  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and numlen is not null and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,pdlen,numlen,catsex order by svspp,year,cruise6,stratum,station,pdlen
Select svspp,year,cruise6,stratum,station,pdlen,COLLCAT,sum(pyamtv) sumpvol  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' group by svspp,year,cruise6,stratum,station,pdlen,COLLCAT order by svspp,year,cruise6,stratum,station,pdlen,COLLCAT
Select distinct svspp,year,cruise6,stratum,station,pdid,pdlen from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and numlen is null and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB'
Select distinct year,cruise6,stratum,station,beglat,beglon  from fhdbs.allfh_feast where pynam <> 'BLOWN' and pynam <> 'PRESERVED' and pynam <> ' ' and svspp='073' and YEAR BETWEEN '1973' AND '2016' and GEOAREA='GB' order by year,cruise6,stratum,station

```


#### Environmental submodels
Information was synthesized entirely from published sources and expert knowledge; no additional data extraction was completed for the environmental submodels.

#### Human dimensions submodels
Recreational fisheries data were extracted from the 2010-2014 MRIP datasets. Original data can be found [here]( data/top10_prim1_common_mode.xlsx) for each region (New England or Mid-Atlantic as defined by states). 

Commercial fishing data was developed as part of the State of the Ecosystem Report, including revenue and food production estimates, with data extraction metodology discussed in the relevant sections of the technical memo. In addition, the Northeast Regional Input/Output Model [@steinback_scott_northeast_2006] was used as the basis for the strength of the employment linkages.

### Data analysis
<!--Text description of analysis methods, similar in structure and detail to a peer-reviewed paper methods section.-->
#### Ecological submodels
Aggregated diet and mortality information was examined to determine the type of link, direction of link, and which links between which groups should be inclded in the conceptual models. Two types of ecological links were defined using food web models: prey links and predation/fishing mortality links. Prey links resulted in positve links between the prey group and the focal group, while predation/fishing mortality links resulted in negative links to the focal group to represent energy flows. The intent was to include only the most important linkages between focal groups and with other groups supporting or causing mortality on focal species groups. Therefore, threshold levels of diet and mortality were established (based on those that would select the top 1-3 prey and predators of each focal group): 10% to include a link (or add a linked group) in the model and 20% to include as a strong link. A Primary Production group was included in each model and linked to pelagic habitat to allow environmental effects on habitat to be connected to the ecologial submodel. Uncertainty for the inclusion of each link and for the magnitude of each link was qualitatively assessed and noted in the [spreadsheet](https://comet.nefsc.noaa.gov/erddap/tabledap/concept_model_2018.html). 

Four habitat categories (Pelagic, Seafloor and Demersal, Nearshore, and Freshwater and Estuarine) were included in ecological submodels as placeholders to be developed further along with habitat-specific research. Expert opinion was used to include the strongest links between each habitat type and each Focal group (noting that across species and life stages, members of these aggregate groups likely occupy many if not all of the habitat types). Link direction and strength were not specified. Environmental drivers were designed to link to habitats, rather than directly to Focal groups, to represent each habitat's important mediation function.

EMAX model groups were aggregated to focal groups for the Georges Bank (GB), Gulf of Maine (GOM) and Mid-Atlantic Bight (MAB) conceptual models according to Table \@ref(tab:groups). "Linked groups" directly support or impact the Focal groups as described above.

```{r groups,eval = T, echo = F}
#read in EMAXconceptualmodgroups.csv and kable it
emaxgroups <- read.csv(file.path(data.dir, "EMAXconceptualmodgroups.csv"))
names(emaxgroups) <- c("Group Type", "Region", "Conceptual model group", "EMAX group(s)", "Notes")
kable(emaxgroups, caption="Relationship between food web model groups and conceptual model focal groups")
```


Ecological submodels were constructed and visualized in Mental Modeler (Fig. \@ref(fig:draftGOMeco)). Here, we show only the Gulf of Maine submodels as examples.

```{r draftGOMeco, fig.cap="Gulf of Maine Ecological submodel", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MM_GoM_Ecological.png'))
```

#### Environmental submodels
Environmental submodels were designed to link key oceanographic processes in each ecosystem production unit to the four general habitat categories (Pelagic, Seafloor and Demersal, Nearshore, and Freshwater and Estuarine) with emphasis on the most important physical processes in each ecosystem based on expert knowledge as supported by literature review. The basis of each submodel were environmental variables observable at management-relevant scales as identified by [WGNARS](http://ices.dk/sites/pub/Publication%20Reports/Expert%20Group%20Report/SSGRSP/2014/WGNARS14.pdf): Surface and Bottom Water Temperature and Salinity, Freshwater Input, and Stratification (as well as sea ice timing and cover, which is not relevant to the northeast US shelf). Key drivers changing these observable variables and thus structuring habitat dynamics in each [Ecological Production Units](#epu) were added to the model using expert consensus. 

Environmental submodels were initially constructed and visualized in Mental Modeler (Fig. \@ref(fig:draftGOMenv)).
```{r draftGOMenv, fig.cap="Gulf of Maine Environmental submodel", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MM_GoM_Climate.png'))
```

#### Human dimensions submodels
The top 3 species from each mode of recreational fishing (shoreside, private boat, party/charter) were used to assess the potential for missing links between the recreational fishing activity and biological focal components. Given the predominance of Mid-Atlantic groundfish in recreational fishing off New England (summer flounder, bluefish, striped bass), a Mid-Atlantic groundfish focal component was added to the Georges Bank EPU model. The magnitude of benefits generated from recreational fishing was scaled to reflect expert knowledge of target species, coupled with the MRIP data highlighted above. Scales were held consistent across the focal components within recreational fishing.

No additional biological focal components were added to the commercial fishing activity, beyond what was developed in the ecological submodel. Benefits derived from commercial fishing were scaled to be consistent with the State of the Ecosystem revenue estimates, as modulated by expert knowledge and additional data sources. For example,the percentage of landings sold as food was used to map fishing activity to the commercial fishery food production objective, and the Northeast Regional Input/Output Model [@steinback_scott_northeast_2006] was used to define the strength of the employment linkages. For profitability, expert knowledge was used to reweight revenue landings, based on ancillary cost data available [@das_chhandita_northeast_2013; @das_chhandita_overview_2014]. Human activities and objectives for the conceptual sub model are defined in @depiper_operationalizing_2017. As shown in Figure \@ref(fig:draftGOMhuman), human dimensions submodels were also initially constructed and visualized in Mental Modeler.

```{r draftGOMhuman, fig.cap="Gulf of Maine Human dimensions submodel", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MM_GoM_Human_Connections.png'))
```

#### Merged models
All links and groups from each submodel were preserved in the full merged model for each system. Mental modeler was used to merge the submodels. Full models were then re-drawn in Dia (http://dia-installer.de/) with color codes for each model component type for improved readability. Examples for each system are below. 

```{r diaGB, fig.cap="Georges Bank conceptual model", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'GBoverview5.png'))
```


```{r diaGOM, fig.cap="Gulf of Maine conceptual model", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'GoMoverview4.png'))
```


```{r diaMAB, fig.cap="Mid-Atlantic Bight conceptual model", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MAB_3.png'))
```

#### Communication tools
The merged models were redrawn for use in communications with the public. These versions lead off the State of the Ecosystem reports for both Fishery Management Councils to provide an overview of linkages between environmental drivers, ecological, and human systems. 

```{r prettyNE, fig.cap="New England conceptual model for public communication", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'GOM_GB_conmod_overview.jpg'))
```

```{r prettyMA, fig.cap="Mid-Atlantic conceptual model for public communication", echo = F, eval = T}

knitr::include_graphics(file.path(image.dir, 'MAB_conmod_overview.jpg'))
```

<!--
What packages or libraries did you use in your work flow?
```{r, echo = T}
sessionInfo(package = NULL)


#Use this to output a detailed list of the package information
current.session <- sessionInfo(package = NULL)
current.session$otherPkgs
```


Include accompanying R code, pseudocode, flow of scripts, and/or link to location of code used in analyses.
```{r, echo = T, eval = F}
# analysis code
```
-->












<!--chapter:end:chapters/conceptualmodels.Rmd-->

# Fish Condition Indicator

**Description**: Relative condition

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018,2019), State of the Ecosystem - Mid-Atlantic (2018,2019) 

**Indicator category**: Database pull with analysis

**Contributor(s)**: Laurel Smith
  
**Data steward**: Laurel Smith, <laurel.smith@noaa.gov>
  
**Point of contact**: Laurel Smith, <laurel.smith@noaa.gov>
  
**Public availability statement**: NEFSC survey data used in these analyses are available upon request (see [BTS metadata](https://inport.nmfs.noaa.gov/inport/item/22560) for access procedures). Derived condition data are available [here](https://comet.nefsc.noaa.gov/erddap/tabledap/gf_condition_soe_v1.html).


## Methods
Relative condition (Kn) was introduced by @Cren1951a as a way to remove the influence of length on condition, and @Blackwell2000 noted that Kn may be useful in detecting prolonged physical stress on a fish populations. Relative condition is calculated as
$Kn = W/W',$ where W is the weight of an individual fish and W' is the predicted length-specific mean weight for the fish population in a given region. "Here, relative condition was calculated for finfish stocks commonly caught on the Northeast Fisheries Science Center’s (NEFSC) autumn and spring bottom trawl surveys, from 1992-present. 

Where data allowed, predicted length-weight parameters were calculated for W’ by species, sex and season over the time period 1992-2012. When sample sizes of individual fish weights and lengths were too low, parameters were calculated for aggregated spring and fall survey data over the same time period. Fall survey relative condition was calculated by sex for those species that exhibited differences in growth between sexes and aggregated across sex for those that did not.

### Data sources
Individual fish lengths (to the nearest 0.5 cm) and weights (grams) were collected on the NEFSC bottom trawl surveys from 1992-present aboard RVs Albatross IV, Delaware II and the Henry B. Bigelow  (see [Survdat](#survdat)). A small number of outlier values were removed when calculating the length-weight parameters.

### Data extraction
Data were extracted from NEFSC's survey database (SVDBS) using SQL. 

SQL query:
```{sql, eval = F, echo = T}
SELECT cruise6,stratum,tow,station,
  year,month,day,time,beglat,beglon,setdepth,
  surftemp,bottemp,
  svspp,sex,length,age,maturity,indid,indwt,stom_volume,stom_wgt, expcatchwt, expcatchnum
from connection to oracle
(select b.cruise6,b.stratum,b.tow,b.station,
  s.est_year year,est_month month,est_day day,
  substr(est_time,1,2)||substr(est_time,4,2) time,
  round(substr(beglat,1,2) + (substr(beglat,3,7)/60),6) beglat,
  round(((substr(beglon,1,2) + (substr(beglon,3,7)/60)) * -1), 6) beglon,
  setdepth,surftemp, bottemp,
  b.svspp,sex,length,age,maturity,indid,indwt,stom_volume,stom_wgt, expcatchwt, expcatchnum
from union_fscs_svbio b, union_fscs_svcat p, union_fscs_svsta s, svdbs_cruises c
where 
  season = &sson and
    b.svspp in ('013','015','023','026','028','032','072','073','074','075','076','077','078','102','103','104','105','106','107','108','121','131','135','141','143','145','155','164','193','197') and
  (b.cruise6=s.cruise6) and
  (c.cruise6=b.cruise6) and
  (p.cruise6=c.cruise6) and
  (p.stratum=b.stratum) and
  (b.stratum=s.stratum) and
  (p.station=b.station) and
  (b.station=s.station) and
  (p.svspp=b.svspp) and
  (p.tow=b.tow) and
  (b.tow=s.tow) );

  %put &sqlxmsg;
  %put &sqlxrc;

create view spp as

select comname, svspp
from connection to oracle
(select comname, svspp
from svspecies_list);

  %put &sqlxmsg;
  %put &sqlxrc;

execute (commit) by oracle;

```

### Data analysis
The following growth curve was fit through individual fish lengths and weights from the NEFSC bottom trawl survey data from 1992-2012 to produce reference length-weight parameters:


$$\textrm{Weight} = e^{Fall_{coef}} * \textrm{Length}^{Fall_{exp}},$$

where length is in cm and weight is in kg. Fall survey data were used where sample sizes allowed for growth curve estimation, otherwise data from spring and fall seasons were combined. 

Individual fish lengths from NEFSC fall bottom trawl survey from 1992-2017 were then used to calculate predicted weights using the reference length-weight parameters. Relative condition (Kn) was calculated annually by species and sex (for sexually dimorphic species) by dividing individual fish weights by the predicted weight. 

The following R code was used in the analysis:
```{r, echo = T, eval = F}
# Length-weight parameter calculation:
function (data, min.n = 25, min.range = 5, data.avail = NA, data.avail.bigelow = NA) 
{
	if(is.null(dim(data.avail))) data.avail <- lw.data.availability(data, min.n, min.range)
	data.avail <- data.avail[apply(data.avail[,2:5], 1, any),]
	if(is.null(dim(data.avail.bigelow)))data.avail.bigelow <- lw.data.availability(data[data$data.source == "Bigelow",], min.n, min.range)
	data.avail.bigelow <- data.avail.bigelow[apply(data.avail.bigelow[,2:5], 1, any),]
	data.spp <- as.numeric(rownames(data.avail[data.avail$sex.season == TRUE,]))
	lw.output <- data.frame(matrix(ncol = 12))
	names(lw.output) <- c("species.name", "species.code", "source", "sex", "season", "slope", "slope.p", "intercept", "intercept.p", "min.length", "max.length", "check.diff")
	for(sp in data.spp){
		sp.data <- lw.data[lw.data$species == sp,]
		sp.name <- unique(as.character(species.names$scientific_name[species.names$svspp == sp]))
		print(sp.name)
#All model
		print("Species Level")
		this.data <- bigelow.test(sp.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], "weight.log~length.log", "species")
		ds <- this.data[[2]]
		this.data <- this.data[[1]]
		if(!is.null(dim(this.data))){
			this.model <- lm(weight.log~length.log, data = this.data)
			model.coefs <- coef(this.model)
			model.summary <- coef(summary(this.model))
			length.range <- range(sp.data$length)
			length.log <- log(seq(length.range[1], length.range[2], by=.5))
			species <- rep(sp, length(length.log))
			predict.length <- data.frame(species, length.log)
			check.diffs <- plot.lw(this.data, this.model, "species", sp.name, predict.length)
			lw.output <- rbind(lw.output, c(sp.name, sp, ds, NA, NA, model.coefs["length.log"], model.summary["length.log", "Pr(>|t|)"], model.coefs["(Intercept)"], model.summary["(Intercept)", "Pr(>|t|)"], length.range[1], length.range[2], check.diffs))
		}
#Sex model
		print("Sex Level")
		this.data <- sp.data[sp.data$sex > 0,]
		model.definition <- "weight.log~length.log * factor(sex)"
		this.data <- bigelow.test(this.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, "sex")
		ds <- this.data[[2]]
		this.data <- this.data[[1]]
		if(!is.null(dim(this.data))){
			this.model <- lm(weight.log~length.log * factor(sex), data = this.data)
			male.range <- range(this.data$length[this.data$sex == 1])
			female.range <- range(this.data$length[this.data$sex == 2])			
			model.summary <- coef(summary(this.model))
			if(any(model.summary[grep("sex", rownames(model.summary)), "Pr(>|t|)"] <= .05)){
				model.coefs <- coef(this.model)
				length.log <- rep(log(seq(length.range[1], length.range[2], by=.5)),2)
				sex <- c(rep(1, length(length.log)/2), rep(2, length(length.log)/2))
				predict.length <- data.frame(length.log, sex)
				check.diffs <- plot.lw(this.data, this.model, "sex", sp.name, predict.length)
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 1, NA, model.coefs["length.log"], model.summary["length.log", "Pr(>|t|)"], model.coefs["(Intercept)"], model.summary["(Intercept)", "Pr(>|t|)"], male.range[1], male.range[2], check.diffs[1]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 2, NA, model.coefs["length.log"] + model.coefs["length.log:factor(sex)2"], model.summary["length.log:factor(sex)2", "Pr(>|t|)"], model.coefs["(Intercept)"] + model.coefs["factor(sex)2"], model.summary["factor(sex)2", "Pr(>|t|)"], female.range[1], female.range[2], check.diffs[2]))
			}
			else{
				print(paste("Model parameters not significantly different for", model.definition))
			}
		}
#Season model
		model.definition <- "weight.log~length.log * factor(season)"
		this.data <- bigelow.test(sp.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, "season")
		ds <- this.data[[2]]
		this.data <- this.data[[1]]
		if(!is.null(dim(this.data))){
			this.model <- lm(weight.log~length.log * factor(season), data = this.data)
			fall.range <- range(this.data$length[this.data$season == "FALL"])
			spring.range <- range(this.data$length[this.data$season == "SPRING"])
			model.summary <- coef(summary(this.model))
			if(any(model.summary[grep("season", rownames(model.summary)), "Pr(>|t|)"] <= .05)){
				model.coefs <- coef(this.model)
				length.log <- rep(log(seq(length.range[1], length.range[2], by=.5)),2)
				season <- c(rep("FALL", length(length.log)/2), rep("SPRING", length(length.log)/2))
				predict.length <- data.frame(length.log, season)
				check.diffs <- plot.lw(this.data, this.model, "season", sp.name, predict.length)
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, NA, "FALL", model.coefs["length.log"], model.summary["length.log", "Pr(>|t|)"], model.coefs["(Intercept)"], model.summary["(Intercept)", "Pr(>|t|)"], fall.range[1], fall.range[2], check.diffs[1]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, NA, "SPRING", model.coefs["length.log"] + model.coefs["length.log:factor(season)SPRING"], model.summary["length.log:factor(season)SPRING", "Pr(>|t|)"], model.coefs["(Intercept)"] + model.coefs["factor(season)SPRING"], model.summary["factor(season)SPRING", "Pr(>|t|)"], spring.range[1], spring.range[2], check.diffs[1]))
			}
			else{
				print(paste("Model parameters not significantly different for", model.definition))
			}
		}
#Sex-Season model
		this.data <- sp.data[sp.data$sex > 0,]
		model.definition <- "weight.log~length.log * factor(sex) * factor(season)"
		this.data <- bigelow.test(this.data, data.avail[as.character(sp),], data.avail.bigelow[as.character(sp),], model.definition, c("sex","season"))
		ds <- this.data[[2]]
		this.data <- this.data[[1]]
		if(!is.null(dim(this.data))){
			this.model <- lm(weight.log~length.log * factor(sex) * factor(season), data = this.data)
			model.summary <- coef(summary(this.model))
			male.fall.range <- range(this.data$length[this.data$season == "FALL" & this.data$sex == 1])
			male.spring.range <- range(this.data$length[this.data$season == "SPRING" & this.data$sex == 1])
			female.fall.range <- range(this.data$length[this.data$season == "FALL" & this.data$sex == 2])
			female.spring.range <- range(this.data$length[this.data$season == "SPRING" & this.data$sex == 2])
			if(any(model.summary[grep("season", rownames(model.summary)), "Pr(>|t|)"] <= .05 | any(model.summary[grep("season", rownames(model.summary)), "Pr(>|t|)"] <= .05))){
				model.coefs <- coef(this.model)
				male.fall.int <- model.coefs["(Intercept)"]
				male.fall.int.p <- model.summary["(Intercept)", "Pr(>|t|)"]
				male.fall.slope <- model.coefs["length.log"]
				male.fall.slope.p <- model.summary["length.log", "Pr(>|t|)"]
				male.spring.int <- male.fall.int + model.coefs["factor(season)SPRING"]
				male.spring.int.p <- model.summary["factor(season)SPRING", "Pr(>|t|)"]
				male.spring.slope <- male.fall.slope + model.coefs["length.log:factor(season)SPRING"]
				male.spring.slope.p <- model.summary["length.log:factor(season)SPRING", "Pr(>|t|)"]
				female.fall.int <- male.fall.int + model.coefs["factor(sex)2"]
				female.fall.int.p <- model.summary["factor(sex)2", "Pr(>|t|)"]
				female.fall.slope <- male.fall.slope + model.coefs["length.log:factor(sex)2"]
				female.fall.slope.p <- model.summary["length.log:factor(sex)2", "Pr(>|t|)"]
				female.spring.int <- male.spring.int + model.coefs["factor(sex)2"] + model.coefs["factor(sex)2:factor(season)SPRING"]
				female.spring.int.p <- model.summary["factor(sex)2:factor(season)SPRING",  "Pr(>|t|)"]
				female.spring.slope <- male.spring.slope + model.coefs["length.log:factor(sex)2"] + model.coefs["length.log:factor(sex)2:factor(season)SPRING"]
				female.spring.slope.p <- model.summary["length.log:factor(sex)2:factor(season)SPRING", "Pr(>|t|)"]
				length.log <- rep(log(seq(length.range[1], length.range[2], by=.5)),4)
				sex <- c(rep("1", length(length.log)/2), rep("2", length(length.log)/2))
				season <- rep(c(rep("FALL", length(length.log)/4), rep("SPRING", length(length.log)/4)),2)
				predict.length <- data.frame(length.log, sex, season)
				check.diffs <- plot.lw(this.data, this.model, c("sex", "season"), sp.name, predict.length)
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 1, "FALL", male.fall.slope, male.fall.slope.p, male.fall.int, male.fall.int.p, male.fall.range[1], male.fall.range[2], check.diffs[1]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 1, "SPRING", male.spring.slope, male.spring.slope.p, male.spring.int, male.spring.int.p, male.spring.range[1], male.spring.range[2],check.diffs[2]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 2, "FALL", female.fall.slope, female.fall.slope.p, female.fall.int, female.fall.int.p, female.fall.range[1], female.fall.range[2],check.diffs[3]))
				lw.output <- rbind(lw.output, c(sp.name, sp, ds, 2, "SPRING", female.spring.slope, female.spring.slope.p, female.spring.int, female.spring.int.p, female.spring.range[1], female.spring.range[2],check.diffs[4]))
			}
			else{
				print(paste("Model parameters not significantly different for", model.definition))
			}
		}
	}
lw.output <- lw.output[!is.na(lw.output$species.code),]
lw.output
}
#Relative Condition:
proc import datafile = "lw_parameters.csv"
 out = LWparams
 dbms = csv
 replace;
 getnames = yes;
run;

data LWparams; set LWparams;
 if LW_SVSPP = 13 then svspp = '013';
 if LW_SVSPP = 15 then svspp = '015';
 if LW_SVSPP = 23 then svspp = '023';
 if LW_SVSPP = 26 then svspp = '026';
 if LW_SVSPP = 28 then svspp = '028';
 if LW_SVSPP = 32 then svspp = '032';
 if LW_SVSPP = 72 then svspp = '072';
 if LW_SVSPP = 73 then svspp = '073';
 if LW_SVSPP = 74 then svspp = '074';
 if LW_SVSPP = 75 then svspp = '075';
 if LW_SVSPP = 76 then svspp = '076';
 if LW_SVSPP = 77 then svspp = '077';
 if LW_SVSPP = 78 then svspp = '078';
 if LW_SVSPP ge 100 then svspp = LW_SVSPP;
 if sexMF = 'M' then sex = '1';
 if sexMF = 'F' then sex = '2';
 if sexMF = ' ' then sex = '0';
 if EXPONENT_FALL = . then EXPONENT_FALL=SEASONLESS_EXPONENT;
 if EXPONENT_SPRING = . then EXPONENT_SPRING=SEASONLESS_EXPONENT;
 if COEFFICIENT_FALL = . then COEFFICIENT_FALL=SEASONLESS_COEFFICIENT;
 if COEFFICIENT_SPRING = . then COEFFICIENT_SPRING=SEASONLESS_COEFFICIENT;

proc sort data=LWparams;
 by svspp sex;

proc sort data=lenwt;
 by svspp sex;

data lwdatpar (keep =cruise6 stratum tow station year month day time beglat beglon setdepth
  surftemp bottemp svspp sex length age maturity indid indwt stom_volume stom_wgt expcatchwt expcatchnum
  COEFFICIENT_SPRING EXPONENT_SPRING COEFFICIENT_FALL EXPONENT_FALL SEASONLESS_COEFFICIENT 
  SEASONLESS_EXPONENT);
 merge lenwt (in=d) LWparams (in=p);
 by svspp sex;

data sortlw; set lwdatpar;
 proc sort; by svspp sex year;

data lwdata; set sortlw;
 if indwt = . then delete;
 if length = . then delete;
 if indwt >0;
 svspp1 = svspp*1;
 indwtg=indwt*1000.0;
 cond=indwtg/(length**3);
 if EXPONENT_FALL gt 0 then predwt = (exp(COEFFICIENT_FALL))*length**EXPONENT_FALL;
 if EXPONENT_FALL = . then predwt = (exp(SEASONLESS_COEFFICIENT))*length**SEASONLESS_EXPONENT;
  if EXPONENT_FALL gt 0 then predwtPK = exp(COEFFICIENT_FALL+(EXPONENT_FALL*log(length)));
 if EXPONENT_FALL = . then predwtPK = exp(SEASONLESS_COEFFICIENT+(SEASONLESS_EXPONENT*log(length)));

***Relative condition;
 RelWt = indwt/predwt*100;

proc sort data=lwdata;
 by svspp1 sex year;
run;

```

### Plotting

```{r condition-factor, eval = T, fig.width = 8, fig.cap = "Normalized condition factors of managed species in the Northeast Large Marine Ecosystem.", message=F, warning=F, fig.height = 7.5}

library(dplyr)
library(tidyr)
library(stringr)

data.dir <- here::here("data")

#Function for normalizing 
ztrans <- function(x){
    meanx <- mean(x, na.rm = T)
    sdx   <- sd(  x, na.rm = T)
    z     <- (x - meanx) / sdx
    return(z)
}

#Get data
load(file.path(data.dir, "SOE_data_erddap.Rdata"))

CF <- SOE.data %>% filter(str_detect(Var, "condition"))

#Processing
CF_mat <- CF %>%
  group_by(Var) %>% 
  mutate(Value = ztrans(Value)) %>% #Normalize
  tidyr::spread(.,Time,Value) %>% #Convert to wide for plotting
  arrange(Var) %>% 
  dplyr::select(-Units, -EPU) 
  

#Figure palette
graph.colors<-colorRampPalette(c('#C6E2FF','#00008B'))

#Main figure
par(mar = c(2, 2, 3, 18), fig = c(0, 1, 0.1, 1))


#Image rotates matrix 90 degrees so use transverse matrix
image(z = t(as.matrix(CF_mat[,2:26])),
      breaks = c(-10, -1, 0, 1, 10), col = graph.colors(4),
      xlim = c(0,1), ylim = c(0-0.037,1+0.037), axes = F,
      xlab = '', ylab = '', useRaster = T)


#Set y axis labels
row_names <- CF_mat$Var
mtext(text = row_names,side = 4, las = 1,
      cex = 0.7, at = seq(0,1,0.0303),
      line = 1)

#Figure key
par(mar = c(0.1, 0.5, 2.25, 0.5),
    fig = c(0.2, 0.5, 0, 0.1),
    new = T)
image(x = seq(0,1,1/4), z = t(matrix(1:4,1,4)), col = graph.colors(4), axes = F, 
      xlim = c(0,1), ylim = c(0,1))
axis(3, at =  seq(0,1,1/4), labels = c(-3, -1, 0, 1, 3), cex = 2)



```


<!--chapter:end:chapters/Condition_indicator.Rmd-->

# Ecological Production Units {#epu}

**Description**: Ecological Production Units

**Found in**: State of the Ecosystem - Gulf of Maine & Georges Bank (2018, 2019), State of the Ecosystem - Mid-Atlantic (2018, 2019) 

**Indicator category**: Extensive analysis, not yet published

<!-- 1. Database pull -->
<!-- 2. Database pull with analysis -->
<!-- 3. Synthesis of published information -->
<!-- 4. Extensive analysis, not yet published -->
<!-- 5. Published methods -->
**Contributor(s)**: Robert Gamble

**Data steward**: NA

**Point of contact**: Robert Gamble, <robert.gamble@noaa.gov>

**Public availability statement**: Ecological production unit (EPU) shapefiles are available [here](https://github.com/NOAA-EDAB/tech-doc/tree/master/gis). More information about source data used to derive EPUs can be found [here](https://www.integratedecosystemassessment.noaa.gov/sites/default/files/pdf/ne-ecological-production-units-paper.pdf).


## Methods
To define ecological production units, we assembled a set of physiographic, oceanographic and biotic variables on the Northeast U.S. Continental Shelf, an area of approximately 264,000 km within the 200 m isobath. The physiographic and hydrographic variables selected have been extensively used in previous analyses of oceanic provinces and regions [e.g @Roff2000]. Primary production estimates have also been widely employed for this purpose in conjunction with physical variables [@Longhurst2007] to define ecological provinces throughout the world ocean. 

We did not include information on higher trophic levels or fishing patterns in our. The biomass and production of higher trophic levels in this region has been sharply perturbed by fishing and other anthropogenic influences. Similarly, fishing patterns are affected by regulatory change, market and economic factors and other external influences. 

Because these malleable patterns of change are often unconnected with underlying productivity, we excluded factors directly related to fishing practices. The physiographic variables considered in this analysis are listed in Table \@ref(tab:epuinputs). They include bathymetry and surficial sediments. The physical oceanographic and hydrographic measurements include sea surface temperature, annual temperature span, and temperature gradient water derived from satellite observations for the period 1998 to 2007. 

### Data sources
Shipboard observations for surface and bottom water temperature and salinity in surveys conducted in spring and fall. Daily sea surface temperature (SST, &deg;C) measurements at 4 km resolution were derived from nighttime scenes composited from the AVHRR sensor on NOAA's polar-orbiting satellites and from NASA's MODIS TERRA and MODIS AQUA sensors. We extracted information for the annual mean SST, temperature span, and temperature gradients from these sources. The latter metric provides information on frontal zone locations. 


```{r epuinputs,  echo = F, include = T, warning = F, message = F, results='asis'}
#Table: (\#label) Variables used in derivation of Ecological Production Units
tab <- '
|Variables|Sampling Method|Units|
|:-----------------------|:-----------------------|:-----------------------|
|Bathymetry|Soundings/Hydroacoustics|Meters|
|Surficial Sediments|Benthic Grab|Krumbian Scale|
|Sea Surface Temperature|Satellite Imagery (4km grid)|&deg;C annual average|
|Sea Surface Temperature|Satellite Imagery (4km grid)|dimensionless|
|Sea Surface Temperature|Satellite Imagery (4km grid)|&deg;C annual average|
|Surface Temperature|Shipboard hydrography (point)|&deg;C (Spring and Fall)|
|Bottom Temperature|Shipboard hydrography (point)|&deg;C (Spring and Fall)|
|Surface Salinity|Shipboard hydrography (point)|psu (Spring and Fall)|
|Bottom Salinity|Shipboard hydrography (point)|psu (Spring and Fall)|
|Stratification|Shipboard hydrography (point)|Sigma-t units (Spring and Fall)|
|Chlorophyll-a|Satellite Imagery (1.25 km grid)|mg/C/m^3^ (annual average)|
|Chlorophyll-a gradient|Satellite Imagery (1.25 km grid)|dimensionless|
|Chlorophyll-a span|Satellite Imagery (1.25 km grid)|mg/C/m^3^ (annual average)|
|Primary Production|Satellite Imagery (1.25 km grid)|gC/m^3^/year (cumulative)|
|Primary Production gradient|Satellite Imagery (1.25 km grid)|dimensionless|
|Primary Production span|Satellite Imagery (1.25 km grid)|gC/m^3^/year (cumulative)|
'
#cat(tab)
df<-read_delim(tab, delim="|")
df<-df[-c(1,2) ,c("Variables","Sampling Method","Units")]
knitr::kable(
  df, booktabs = TRUE,
  caption = 'Variables used in derivation of Ecological Production Units.'
)
```


The biotic measurements included satellite-derived estimates of chlorophyll *a* (CHLa) mean concentration, annual span, and CHLa gradients and related measures of primary production. Daily merged SeaWiFS/MODIS-Aqua CHLa (CHL, mg m^-3^) and SeaiWiFS photosynthetically available radiation (PAR, Einsteins m^-2^ d^-1^) scenes at 1.25 km resolution were obtained from NASA Ocean Biology Processing Group. 

### Data extraction
NA

### Data analysis
In all cases, we standardized the data to common spatial units by taking annual means of each observation type within spatial units of 10' latitude by 10' longitude to account for the disparate spatial and temporal scales at which these observations are taken. There are over 1000 spatial cells in this analysis. Shipboard sampling used to obtain direct hydrographic measurements is constrained by a minimum sampling depth of 27 m specified on the basis of prescribed safe operating procedures. As a result nearshore waters are not fully represented in our initial specifications of ecological production units. 

The size of the spatial units employed further reflects a compromise between retaining spatial detail and minimizing the need for spatial interpolation of some data sets. For shipboard data sets characterized by relatively coarse spatial resolution, where necessary, we first constructed an interpolated map using an inverse distance weighting function before including it in the analysis. Although alternative interpolation schemes based on geostatistical approaches are possible, we considered the inverse distance weighting function to be both tractable and robust for this application. 

We first employed a spatial principal components analysis [PCA; e.g. @Pielou1984; @Legendre1998] to examine the multivariate structure of the data and to account for any inter-correlations among the variables to be used in subsequent analysis. The variables included in the analysis exhibited generally skewed distributions and we therefore transformed each to natural logarithms prior to analysis. 

The PCA was performed on the correlation matrix of the transformed observations. We selected the eigenvectors associated with eigenvalues of the dispersion matrix with scores greater than 1.0 [the Kaiser-Guttman criterion; @Legendre1998] for all subsequent analysis. These eigenvectors represent orthogonal linear combinations of the original variables used in the analysis. 

We delineated ecological subunits by applying a disjoint cluster based on Euclidean distances using the K-means procedure [@Legendre1998] on the principal component scores The use of non-independent variables can strongly influence the results of classification analyses of this type [@Pielou1984], hence the interest in using the PCA results in the cluster. 

The eigenvectors were represented as standard normal deviates. We used a Pseudo-F Statistic described by @Milligan1985 to objectively define the number of clusters to use in the analysis. The general approach employed is similar to that of @Host1996 for the development of regional ecosystem classifications for terrestrial systems.

After the analyses were done, we next considered options for interpolation of nearshore boundaries resulting from depth-related constraints on shipboard observations. For this, we relied on information from satellite imagery. For the missing nearshore areas in the Gulf of Maine and Mid-Atlantic Bight, the satellite information for chlorophyll concentration and sea surface temperature indicated a direct extension from adjacent observations. For the Nantucket Shoals region south of Cape Cod, similarities in tidal mixing patterns reflected in chlorophyll and temperature observations indicated an affinity with Georges Bank and the boundaries were changed accordingly.

Finally, we next considered consolidation of ecological subareas so that nearshore regions are considered to be special zones nested within the adjacent shelf regions. Similar consideration led to nesting the continental slope regions within adjacent shelf regions in the Mid-Atlantic and Georges Bank regions. This led to four major units: Mid-Atlantic Bight, Georges Bank, Western-Central Gulf of Maine (simply "Gulf of Maine" in the SOE), and Scotian Shelf-Eastern Gulf of Maine. As the State of the Ecosystem reports are specific to FMC managed regions, the Scotian Shelf-Eastern Gulf of Maine EPU is not considered in SOE indicator analyses. 


```{r EPUmap, fig.cap="Map of the four Ecological Production Units, including the Mid-Atlantic Bight (light blue), Georges Bank (red), Western-Central Gulf of Maine (or Gulf of Maine; green), and Scotian Shelf-Eastern Gulf of Maine (dark blue)", fig.align='center', echo = F}

knitr::include_graphics(file.path(image.dir,"EPUs.jpg"))

```


<!--chapter:end:chapters/EPU.Rmd-->

